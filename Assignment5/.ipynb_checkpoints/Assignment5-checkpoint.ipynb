{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rl.iterate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d6e852264d99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"..\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0miterate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_approx\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rl.iterate'"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass, replace, field\n",
    "import itertools\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from scipy.interpolate import BSpline,splrep\n",
    "from scipy.stats import norm\n",
    "from typing import (Callable, Dict, Generic, Iterator, Iterable, List,\n",
    "                    Mapping, Optional, Sequence, Tuple, TypeVar)\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "from rl.iterate import iterate\n",
    "from rl.function_approx import *\n",
    "X = TypeVar('X')\n",
    "SMALL_NUM = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question:\n",
    "<br>1) For Q1 Univeriate Basis Spline, I'm not that familliar with Basis Spline Regression. But why are there so many knots in the fitted model, even though most of them are 0? And I find that different batchs of (X_vals,y_vals) given us models that have relatively large difference in knots and coeffs? May I know why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FunctionApprox' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0313eb65e8e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrozen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mUniBSplineApprox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFunctionApprox\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \"\"\"\n\u001b[0;32m      4\u001b[0m         \u001b[0mUniveriate\u001b[0m \u001b[0mB\u001b[0m \u001b[0mSpline\u001b[0m \u001b[0mApproximation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FunctionApprox' is not defined"
     ]
    }
   ],
   "source": [
    "@dataclass(frozen=True)\n",
    "class UniBSplineApprox(FunctionApprox[X]):\n",
    "    \"\"\"\n",
    "        Univeriate B Spline Approximation\n",
    "    \"\"\"\n",
    "    degree: int\n",
    "    knots: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    coeffs: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def create(k:int) -> UniBSplineApprox[X]:\n",
    "        return UniBSplineApprox(degree=k )\n",
    "\n",
    "    def evaluate(self, x_values_seq: Iterable[X]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "            Given current knots,coeffs, and iter X, calculate Y.\n",
    "        \"\"\"\n",
    "        return BSpline(self.knots, self.coeffs, self.degree)(x_values_seq)\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]]\n",
    "    ) -> UniBSplineApprox[X]:\n",
    "        \"\"\"\n",
    "            Given data (X,Y),update knot and coeff.\n",
    "        \"\"\"\n",
    "        x_vals, y_vals = zip(*xy_vals_seq)\n",
    "        sorted_pairs: Sequence[Tuple[float, float]] = \\\n",
    "            sorted(zip(x_vals, y_vals), key=itemgetter(0))\n",
    "        new_knots, new_coeffs, _ = splrep(\n",
    "            [f for f, _ in sorted_pairs],\n",
    "            [y for _, y in sorted_pairs],\n",
    "            k=self.degree\n",
    "        )\n",
    "        return replace(\n",
    "            self,\n",
    "            knots=new_knots,\n",
    "            coeffs=new_coeffs\n",
    "        )\n",
    "    def representational_gradient(self, x_value: X) -> BSplineApprox[X]:\n",
    "        feature_val: float = self.feature_function(x_value)\n",
    "        eps: float = 1e-6\n",
    "        one_hots: np.array = np.eye(len(self.coeffs))\n",
    "        return replace(\n",
    "            self,\n",
    "            coeffs=np.array([(\n",
    "                BSpline(\n",
    "                    self.knots,\n",
    "                    c + one_hots[i] * eps,\n",
    "                    self.degree\n",
    "                )(feature_val) -\n",
    "                BSpline(\n",
    "                    self.knots,\n",
    "                    c - one_hots[i] * eps,\n",
    "                    self.degree\n",
    "                )(feature_val)\n",
    "            ) / (2 * eps) for i, c in enumerate(self.coeffs)]))\n",
    "\n",
    "    def solve(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]],\n",
    "        error_tolerance: Optional[float] = None\n",
    "    ) -> BSplineApprox[X]:\n",
    "        return self.update(xy_vals_seq)\n",
    "\n",
    "    def within(self, other: FunctionApprox[X], tolerance: float) -> bool:\n",
    "        \"\"\"\n",
    "         if both coeff and knot are within tol then return True\n",
    "        \"\"\"\n",
    "        if isinstance(other, UniBSplineApprox):\n",
    "            return \\\n",
    "                np.all(np.abs(self.knots - other.knots) <= tolerance).item() \\\n",
    "                and \\\n",
    "                np.all(np.abs(self.coeffs - other.coeffs) <= tolerance).item()\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=X^3+error\n",
    "def example_model_data_generator() -> Iterator[Tuple[Triple, float]]:\n",
    "    d = norm(loc=0., scale=0.01)\n",
    "    while True:\n",
    "        x_val: float = np.random.rand()\n",
    "        y_val: float =  x_val**3+d.rvs(size=1)[0]\n",
    "        yield (x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_seq_generator(\n",
    "    data_generator: Iterator[Tuple[Triple, float]],\n",
    "    num_pts: int\n",
    "    ) -> Iterator[DataSeq]:\n",
    "    while True:\n",
    "        pts: DataSeq = list(islice(data_generator, num_pts))\n",
    "        yield pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UniBSplineApprox' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ce1f6af39b3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mCubicModel_0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mUniBSplineApprox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'UniBSplineApprox' is not defined"
     ]
    }
   ],
   "source": [
    "CubicModel_0=UniBSplineApprox.create(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pts=100\n",
    "data_gen=example_model_data_generator()\n",
    "data_flow: Iterator[DataSeq] = data_seq_generator(\n",
    "    data_gen,\n",
    "    num_pts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CubicModel_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-bb70190efed8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mCubicModel_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCubicModel_0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_flow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'CubicModel_0' is not defined"
     ]
    }
   ],
   "source": [
    "CubicModel_1=CubicModel_0.solve(next(data_flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CubicModel_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5c9c20a26a57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mCubicModel_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCubicModel_0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_flow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'CubicModel_0' is not defined"
     ]
    }
   ],
   "source": [
    "CubicModel_2=CubicModel_0.solve(next(data_flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CubicModel_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ee92f8d454a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mCubicModel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCubicModel_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'CubicModel_1' is not defined"
     ]
    }
   ],
   "source": [
    "CubicModel_1.within(CubicModel_2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = zip(*next(data_flow))\n",
    "x_test, y_test = zip(*next(data_flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CubicModel_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-153b466206fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCubicModel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mse=\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CubicModel_1' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred=CubicModel_1.evaluate(x_test)\n",
    "print(\"mse=\",np.mean((y_pred-y_test)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-327816c7791d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Actual\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Predicted\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Y_test vs Y_pred\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x_test,y_test,label=\"Actual\")\n",
    "plt.scatter(x_test,y_pred,label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.title(\"Y_test vs Y_pred\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rl.distribution'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1d3aa2371664>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConstant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_approx\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFunctionApprox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0miterate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rl.distribution'"
     ]
    }
   ],
   "source": [
    "'''Approximate dynamic programming algorithms are variations on\n",
    "dynamic programming algorithms that can work with function\n",
    "approximations rather than exact representations of the process's\n",
    "state space.\n",
    "'''\n",
    "\n",
    "from typing import Iterator, Mapping, Tuple, TypeVar, Sequence, List\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "from rl.distribution import Distribution, Constant\n",
    "from rl.function_approx import FunctionApprox\n",
    "from rl.iterate import iterate\n",
    "from rl.markov_process import (FiniteMarkovRewardProcess, MarkovRewardProcess,\n",
    "                               RewardTransition)\n",
    "from rl.markov_decision_process import (FiniteMarkovDecisionProcess, Policy,\n",
    "                                        MarkovDecisionProcess,\n",
    "                                        StateActionMapping)\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "\n",
    "# A representation of a value function for a finite MDP with states of\n",
    "# type S\n",
    "V = Mapping[S, float]\n",
    "\n",
    "\n",
    "def evaluate_finite_mrp(\n",
    "        mrp: FiniteMarkovRewardProcess[S],\n",
    "        γ: float,\n",
    "        approx_0: FunctionApprox[S]\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "\n",
    "    '''Iteratively calculate the value function for the give finite Markov\n",
    "    Reward Process, using the given FunctionApprox to approximate the\n",
    "    value function at each step.\n",
    "    '''\n",
    "    def update(v: FunctionApprox[S]) -> FunctionApprox[S]:\n",
    "        vs: np.ndarray = v.evaluate(mrp.non_terminal_states)\n",
    "        updated: np.ndarray = mrp.reward_function_vec + γ * \\\n",
    "            mrp.get_transition_matrix().dot(vs)\n",
    "        return v.update(zip(mrp.states(), updated))\n",
    "\n",
    "    return iterate(update, approx_0)\n",
    "\n",
    "\n",
    "def evaluate_mrp(\n",
    "    mrp: MarkovRewardProcess[S],\n",
    "    γ: float,\n",
    "    approx_0: FunctionApprox[S],\n",
    "    non_terminal_states_distribution: Distribution[S],\n",
    "    num_state_samples: int\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "\n",
    "    '''Iteratively calculate the value function for the given Markov Reward\n",
    "    Process, using the given FunctionApprox to approximate the value function\n",
    "    at each step for a random sample of the process' non-terminal states.\n",
    "    '''\n",
    "    def update(v: FunctionApprox[S]) -> FunctionApprox[S]:\n",
    "        nt_states: Sequence[S] = non_terminal_states_distribution.sample_n(\n",
    "            num_state_samples\n",
    "        )\n",
    "\n",
    "        def return_(s_r: Tuple[S, float]) -> float:\n",
    "            s1, r = s_r\n",
    "            return r + γ * v.evaluate([s1]).item()\n",
    "\n",
    "        return v.update(\n",
    "            [(s, mrp.transition_reward(s).expectation(return_))\n",
    "             for s in nt_states]\n",
    "        )\n",
    "\n",
    "    return iterate(update, approx_0)\n",
    "\n",
    "\n",
    "def value_iteration_finite(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "    γ: float,\n",
    "    approx_0: FunctionApprox[S]\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "    '''Iteratively calculate the Optimal Value function for the given finite\n",
    "    Markov Decision Process, using the given FunctionApprox to approximate the\n",
    "    Optimal Value function at each step\n",
    "    '''\n",
    "    def update(v: FunctionApprox[S]) -> FunctionApprox[S]:\n",
    "\n",
    "        def return_(s_r: Tuple[S, float]) -> float:\n",
    "            s1, r = s_r\n",
    "            return r + γ * v.evaluate([s1]).item()\n",
    "\n",
    "        return v.update(\n",
    "            [(\n",
    "                s,\n",
    "                max(mdp.mapping[s][a].expectation(return_)\n",
    "                    for a in mdp.actions(s))\n",
    "            ) for s in mdp.non_terminal_states]\n",
    "        )\n",
    "\n",
    "    return iterate(update, approx_0)\n",
    "\n",
    "\n",
    "def value_iteration(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    γ: float,\n",
    "    approx_0: FunctionApprox[S],\n",
    "    non_terminal_states_distribution: Distribution[S],\n",
    "    num_state_samples: int\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "    '''Iteratively calculate the Optimal Value function for the given\n",
    "    Markov Decision Process, using the given FunctionApprox to approximate the\n",
    "    Optimal Value function at each step for a random sample of the process'\n",
    "    non-terminal states.\n",
    "    '''\n",
    "    def update(v: FunctionApprox[S]) -> FunctionApprox[S]:\n",
    "        nt_states: Sequence[S] = non_terminal_states_distribution.sample_n(\n",
    "            num_state_samples\n",
    "        )\n",
    "\n",
    "        def return_(s_r: Tuple[S, float]) -> float:\n",
    "            s1, r = s_r\n",
    "            return r + γ * v.evaluate([s1]).item()\n",
    "\n",
    "        return v.update(\n",
    "            [(s, max(mdp.step(s, a).expectation(return_)\n",
    "                     for a in mdp.actions(s)))\n",
    "             for s in nt_states]\n",
    "        )\n",
    "\n",
    "    return iterate(update, approx_0)\n",
    "\n",
    "\n",
    "def backward_evaluate_finite(\n",
    "    step_f0_pairs: Sequence[Tuple[RewardTransition[S], FunctionApprox[S]]],\n",
    "    γ: float\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "    '''Evaluate the given finite Markov Reward Process using backwards\n",
    "    induction, given that the process stops after limit time steps.\n",
    "    '''\n",
    "\n",
    "    v: List[FunctionApprox[S]] = []\n",
    "    num_steps: int = len(step_f0_pairs)\n",
    "\n",
    "    for i, (step, approx0) in enumerate(reversed(step_f0_pairs)):\n",
    "\n",
    "        def return_(s_r: Tuple[S, float], i=i) -> float:\n",
    "            s1, r = s_r\n",
    "            return r + γ * (v[i-1].evaluate([s1]).item() if i > 0 and\n",
    "                            step_f0_pairs[num_steps - i][0][s1] is not None\n",
    "                            else 0.)\n",
    "\n",
    "        v.append(\n",
    "            approx0.solve([(s, res.expectation(return_))\n",
    "                           for s, res in step.items() if res is not None])\n",
    "        )\n",
    "\n",
    "    return reversed(v)\n",
    "\n",
    "\n",
    "MRP_FuncApprox_Distribution = \\\n",
    "    Tuple[MarkovRewardProcess[S], FunctionApprox[S], Distribution[S]]\n",
    "\n",
    "\n",
    "def backward_evaluate(\n",
    "    mrp_f0_mu_triples: Sequence[MRP_FuncApprox_Distribution[S]],\n",
    "    γ: float,\n",
    "    num_state_samples: int,\n",
    "    error_tolerance: float\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "    '''Evaluate the given finite Markov Reward Process using backwards\n",
    "    induction, given that the process stops after limit time steps, using\n",
    "    the given FunctionApprox for each time step for a random sample of the\n",
    "    time step's states.\n",
    "    '''\n",
    "    v: List[FunctionApprox[S]] = []\n",
    "\n",
    "    num_steps: int = len(mrp_f0_mu_triples)\n",
    "\n",
    "    for i, (mrp, approx0, mu) in enumerate(reversed(mrp_f0_mu_triples)):\n",
    "\n",
    "        def return_(s_r: Tuple[S, float], i=i) -> float:\n",
    "            s1, r = s_r\n",
    "            return r + γ * (v[i-1].evaluate([s1]).item() if i > 0 and not\n",
    "                            mrp_f0_mu_triples[num_steps - i][0].is_terminal(s1)\n",
    "                            else 0.)\n",
    "\n",
    "        v.append(\n",
    "            approx0.solve(\n",
    "                [(s, mrp.transition_reward(s).expectation(return_))\n",
    "                 for s in mu.sample_n(num_state_samples)\n",
    "                 if not mrp.is_terminal(s)],\n",
    "                error_tolerance\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return reversed(v)\n",
    "\n",
    "\n",
    "def back_opt_vf_and_policy_finite(\n",
    "    step_f0s: Sequence[Tuple[StateActionMapping[S, A], FunctionApprox[S]]],\n",
    "    γ: float,\n",
    ") -> Iterator[Tuple[FunctionApprox[S], Policy[S, A]]]:\n",
    "    '''Use backwards induction to find the optimal value function and optimal\n",
    "    policy at each time step\n",
    "    '''\n",
    "    vp: List[Tuple[FunctionApprox[S], Policy[S, A]]] = []\n",
    "\n",
    "    num_steps: int = len(step_f0s)\n",
    "\n",
    "    for i, (step, approx0) in enumerate(reversed(step_f0s)):\n",
    "\n",
    "        def return_(s_r: Tuple[S, float], i=i) -> float:\n",
    "            s1, r = s_r\n",
    "            return r + γ * (vp[i-1][0].evaluate([s1]).item() if i > 0 and\n",
    "                            step_f0s[num_steps - i][0][s1] is not None else 0.)\n",
    "\n",
    "        this_v = approx0.solve(\n",
    "            [(s, max(res.expectation(return_)\n",
    "                     for a, res in actions_map.items()))\n",
    "             for s, actions_map in step.items() if actions_map is not None]\n",
    "        )\n",
    "\n",
    "        class ThisPolicy(Policy[S, A]):\n",
    "            def act(self, state: S) -> Constant[A]:\n",
    "                return Constant(max(\n",
    "                    ((res.expectation(return_), a)\n",
    "                     for a, res in step[state].items()),\n",
    "                    key=itemgetter(0)\n",
    "                )[1])\n",
    "\n",
    "        vp.append((this_v, ThisPolicy()))\n",
    "\n",
    "    return reversed(vp)\n",
    "\n",
    "\n",
    "MDP_FuncApproxV_Distribution = Tuple[\n",
    "    MarkovDecisionProcess[S, A],\n",
    "    FunctionApprox[S],\n",
    "    Distribution[S]\n",
    "]\n",
    "\n",
    "\n",
    "def back_opt_vf_and_policy(\n",
    "    mdp_f0_mu_triples: Sequence[MDP_FuncApproxV_Distribution[S, A]],\n",
    "    γ: float,\n",
    "    num_state_samples: int,\n",
    "    error_tolerance: float\n",
    ") -> Iterator[Tuple[FunctionApprox[S], Policy[S, A]]]:\n",
    "    '''Use backwards induction to find the optimal value function and optimal\n",
    "    policy at each time step, using the given FunctionApprox for each time step\n",
    "    for a random sample of the time step's states.\n",
    "    '''\n",
    "    vp: List[Tuple[FunctionApprox[S], Policy[S, A]]] = []\n",
    "\n",
    "    num_steps: int = len(mdp_f0_mu_triples)\n",
    "\n",
    "    for i, (mdp, approx0, mu) in enumerate(reversed(mdp_f0_mu_triples)):\n",
    "\n",
    "        def return_(s_r: Tuple[S, float], i=i) -> float:\n",
    "            s1, r = s_r\n",
    "            return r + γ * (vp[i-1][0].evaluate([s1]).item() if i > 0 and not\n",
    "                            mdp_f0_mu_triples[num_steps - i][0].is_terminal(s1)\n",
    "                            else 0.)\n",
    "\n",
    "        this_v = approx0.solve(\n",
    "            [(s, max(mdp.step(s, a).expectation(return_)\n",
    "                     for a in mdp.actions(s)))\n",
    "             for s in mu.sample_n(num_state_samples)\n",
    "             if not mdp.is_terminal(s)],\n",
    "            error_tolerance\n",
    "        )\n",
    "\n",
    "        class ThisPolicy(Policy[S, A]):\n",
    "            def act(self, state: S) -> Constant[A]:\n",
    "                return Constant(max(\n",
    "                    ((mdp.step(state, a).expectation(return_), a)\n",
    "                     for a in mdp.actions(state)),\n",
    "                    key=itemgetter(0)\n",
    "                )[1])\n",
    "\n",
    "        vp.append((this_v, ThisPolicy()))\n",
    "\n",
    "    return reversed(vp)\n",
    "\n",
    "\n",
    "MDP_FuncApproxQ_Distribution = Tuple[\n",
    "    MarkovDecisionProcess[S, A],\n",
    "    FunctionApprox[Tuple[S, A]],\n",
    "    Distribution[S]\n",
    "]\n",
    "\n",
    "\n",
    "def back_opt_qvf(\n",
    "    mdp_f0_mu_triples: Sequence[MDP_FuncApproxQ_Distribution[S, A]],\n",
    "    γ: float,\n",
    "    num_state_samples: int,\n",
    "    error_tolerance: float\n",
    ") -> Iterator[FunctionApprox[Tuple[S, A]]]:\n",
    "    '''Use backwards induction to find the optimal q-value function  policy at\n",
    "    each time step, using the given FunctionApprox (for Q-Value) for each time\n",
    "    step for a random sample of the time step's states.\n",
    "    '''\n",
    "    horizon: int = len(mdp_f0_mu_triples)\n",
    "    qvf: List[FunctionApprox[Tuple[S, A]]] = []\n",
    "\n",
    "    num_steps: int = len(mdp_f0_mu_triples)\n",
    "\n",
    "    for i, (mdp, approx0, mu) in enumerate(reversed(mdp_f0_mu_triples)):\n",
    "\n",
    "        def return_(s_r: Tuple[S, float], i=i) -> float:\n",
    "            s1, r = s_r\n",
    "            return r + γ * (\n",
    "                max(qvf[i-1].evaluate([(s1, a)]).item()\n",
    "                    for a in mdp_f0_mu_triples[horizon - i][0].actions(s1))\n",
    "                if i > 0 and\n",
    "                not mdp_f0_mu_triples[num_steps - i][0].is_terminal(s1)\n",
    "                else 0.\n",
    "            )\n",
    "\n",
    "        this_qvf = approx0.solve(\n",
    "            [((s, a), mdp.step(s, a).expectation(return_))\n",
    "             for s in mu.sample_n(num_state_samples)\n",
    "             if not mdp.is_terminal(s) for a in mdp.actions(s)],\n",
    "            error_tolerance\n",
    "        )\n",
    "\n",
    "        qvf.append(this_qvf)\n",
    "\n",
    "    return reversed(qvf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
