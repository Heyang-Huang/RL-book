{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Generator,Callable,Optional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "from rl.markov_process import MarkovRewardProcess,FiniteMarkovProcess,MarkovProcess\n",
    "from rl.markov_process import FiniteMarkovRewardProcess\n",
    "from rl.markov_process import Transition,TransitionStep,ReturnStep\n",
    "from rl.markov_process import RewardTransition\n",
    "from rl.distribution import Constant,Categorical,SampledDistribution,Distribution\n",
    "from rl.gen_utils.common_funcs import get_logistic_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rl.iterate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3c47229e5ef6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                     Mapping, Optional, Sequence, Tuple, TypeVar)\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0miterate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rl.iterate'"
     ]
    }
   ],
   "source": [
    "'''An interface for different kinds of function approximations\n",
    "(tabular, linear, DNN... etc), with several implementations.'''\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass, replace, field\n",
    "import itertools\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from scipy.interpolate import splrep, BSpline\n",
    "from typing import (Callable, Dict, Generic, Iterator, Iterable, List,\n",
    "                    Mapping, Optional, Sequence, Tuple, TypeVar)\n",
    "\n",
    "import rl.iterate as iterate\n",
    "\n",
    "X = TypeVar('X')\n",
    "SMALL_NUM = 1e-6\n",
    "\n",
    "\n",
    "class FunctionApprox(ABC, Generic[X]):\n",
    "    '''Interface for function approximations.\n",
    "    An object of this class approximates some function X ↦ ℝ in a way\n",
    "    that can be evaluated at specific points in X and updated with\n",
    "    additional (X, ℝ) points.\n",
    "    '''\n",
    "\n",
    "    @abstractmethod\n",
    "    def representational_gradient(self, x_value: X) -> FunctionApprox[X]:\n",
    "        '''Computes the gradient of the self FunctionApprox with respect\n",
    "        to the parameters in the internal representation of the\n",
    "        FunctionApprox, i.e., computes Gradient with respect to internal\n",
    "        parameters of expected value of y for the input x, where the\n",
    "        expectation is with respect tp the FunctionApprox's model of\n",
    "        the probability distribution of y|x. The gradient is output\n",
    "        in the form of a FunctionApprox whose internal parameters are\n",
    "        equal to the gradient values.\n",
    "        '''\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self, x_values_seq: Iterable[X]) -> np.ndarray:\n",
    "        '''Computes expected value of y for each x in\n",
    "        x_values_seq (with the probability distribution\n",
    "        function of y|x estimated as FunctionApprox)\n",
    "        '''\n",
    "\n",
    "    def __call__(self, x_value: X) -> float:\n",
    "        return self.evaluate([x_value]).item()\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]]\n",
    "    ) -> FunctionApprox[X]:\n",
    "\n",
    "        '''Update the internal parameters of the FunctionApprox\n",
    "        based on incremental data provided in the form of (x,y)\n",
    "        pairs as a xy_vals_seq data structure\n",
    "        '''\n",
    "\n",
    "    @abstractmethod\n",
    "    def solve(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]],\n",
    "        error_tolerance: Optional[float] = None\n",
    "    ) -> FunctionApprox[X]:\n",
    "        '''Assuming the entire data set of (x,y) pairs is available\n",
    "        in the form of the given input xy_vals_seq data structure,\n",
    "        solve for the internal parameters of the FunctionApprox\n",
    "        such that the internal parameters are fitted to xy_vals_seq.\n",
    "        Since this is a best-fit, the internal parameters are fitted\n",
    "        to within the input error_tolerance (where applicable, since\n",
    "        some methods involve a direct solve for the fit that don't\n",
    "        require an error_tolerance)\n",
    "        '''\n",
    "\n",
    "    @abstractmethod\n",
    "    def within(self, other: FunctionApprox[X], tolerance: float) -> bool:\n",
    "        '''Is this function approximation within a given tolerance of\n",
    "        another function approximation of the same type?\n",
    "        '''\n",
    "\n",
    "    def argmax(self, xs: Iterable[X]) -> X:\n",
    "        '''Return the input X that maximizes the function being approximated.\n",
    "        Arguments:\n",
    "          xs -- list of inputs to evaluate and maximize, cannot be empty\n",
    "        Returns the X that maximizes the function this approximates.\n",
    "        '''\n",
    "        return list(xs)[np.argmax(self.evaluate(xs))]\n",
    "\n",
    "    def rmse(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]]\n",
    "    ) -> float:\n",
    "        '''The Root-Mean-Squared-Error between FunctionApprox's\n",
    "        predictions (from evaluate) and the associated (supervisory)\n",
    "        y values\n",
    "        '''\n",
    "        x_seq, y_seq = zip(*xy_vals_seq)\n",
    "        errors: np.ndarray = self.evaluate(x_seq) - np.array(y_seq)\n",
    "        return np.sqrt(np.mean(errors * errors))\n",
    "\n",
    "    def iterate_updates(\n",
    "        self,\n",
    "        xy_seq_stream: Iterator[Iterable[Tuple[X, float]]]\n",
    "    ) -> Iterator[FunctionApprox[X]]:\n",
    "        '''Given a stream (Iterator) of data sets of (x,y) pairs,\n",
    "        perform a series of incremental updates to the internal\n",
    "        parameters (using update method), with each internal\n",
    "        parameter update done for each data set of (x,y) pairs in the\n",
    "        input stream of xy_seq_stream\n",
    "        '''\n",
    "        return iterate.accumulate(\n",
    "            xy_seq_stream,\n",
    "            lambda fa, xy: fa.update(xy),\n",
    "            initial=self\n",
    "        )\n",
    "\n",
    "    def representational_gradient_stream(\n",
    "        self,\n",
    "        x_values_seq: Iterable[X]\n",
    "    ) -> Iterator[FunctionApprox[X]]:\n",
    "        for x_val in x_values_seq:\n",
    "            yield self.representational_gradient(x_val)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Dynamic(FunctionApprox[X]):\n",
    "    '''A FunctionApprox that works exactly the same as exact dynamic\n",
    "    programming. Each update for a value in X replaces the previous\n",
    "    value at X altogether.\n",
    "\n",
    "    Fields:\n",
    "    values_map -- mapping from X to its approximated value\n",
    "    '''\n",
    "\n",
    "    values_map: Mapping[X, float]\n",
    "\n",
    "    def representational_gradient(self, x_value: X) -> Dynamic[X]:\n",
    "        return Dynamic({x_value: 1.0})\n",
    "\n",
    "    def evaluate(self, x_values_seq: Iterable[X]) -> np.ndarray:\n",
    "        '''Evaluate the function approximation by looking up the value in the\n",
    "        mapping for each state.\n",
    "\n",
    "        Will raise an error if an X value has not been seen before and\n",
    "        was not initialized.\n",
    "\n",
    "        '''\n",
    "        return np.array([self.values_map[x] for x in x_values_seq])\n",
    "\n",
    "    def update(self, xy_vals_seq: Iterable[Tuple[X, float]]) -> Dynamic[X]:\n",
    "        '''Update each X value by replacing its saved Y with a new one. Pairs\n",
    "        later in the list take precedence over pairs earlier in the\n",
    "        list.\n",
    "\n",
    "        '''\n",
    "        new_map = dict(self.values_map)\n",
    "        for x, y in xy_vals_seq:\n",
    "            new_map[x] = y\n",
    "\n",
    "        return replace(self, values_map=new_map)\n",
    "\n",
    "    def solve(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]],\n",
    "        error_tolerance: Optional[float] = None\n",
    "    ) -> Dynamic[X]:\n",
    "        return replace(self, value_map=dict(xy_vals_seq))\n",
    "\n",
    "    def within(self, other: FunctionApprox[X], tolerance: float) -> bool:\n",
    "        '''This approximation is within a tolerance of another if the value\n",
    "        for each X in both approximations is within the given\n",
    "        tolerance.\n",
    "\n",
    "        Raises an error if the other approximation is missing states\n",
    "        that this approximation has.\n",
    "\n",
    "        '''\n",
    "        if not isinstance(other, Dynamic):\n",
    "            return False\n",
    "\n",
    "        return all(abs(self.values_map[s] - other.values_map[s]) <= tolerance\n",
    "                   for s in self.values_map)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Tabular(FunctionApprox[X]):\n",
    "    '''Approximates a function with a discrete domain (`X'), without any\n",
    "    interpolation. The value for each `X' is maintained as a weighted\n",
    "    mean of observations by recency (managed by\n",
    "    `count_to_weight_func').\n",
    "\n",
    "    In practice, this means you can use this to approximate a function\n",
    "    with a learning rate α(n) specified by count_to_weight_func.\n",
    "\n",
    "    If `count_to_weight_func' always returns 1, this behaves the same\n",
    "    way as `Dynamic'.\n",
    "\n",
    "    Fields:\n",
    "    values_map -- mapping from X to its approximated value\n",
    "    counts_map -- how many times a given X has been updated\n",
    "    count_to_weight_func -- function for how much to weigh an update\n",
    "      to X based on the number of times that X has been updated\n",
    "\n",
    "    '''\n",
    "\n",
    "    values_map: Mapping[X, float] = field(default_factory=lambda: {})\n",
    "    counts_map: Mapping[X, int] = field(default_factory=lambda: {})\n",
    "    count_to_weight_func: Callable[[int], float] = \\\n",
    "        field(default_factory=lambda: lambda n: 1.0 / n)\n",
    "\n",
    "    def representational_gradient(self, x_value: X) -> Tabular[X]:\n",
    "        return Tabular({x_value: 1.0})\n",
    "\n",
    "    def evaluate(self, x_values_seq: Iterable[X]) -> np.ndarray:\n",
    "        '''Evaluate the approximation at each given X.\n",
    "\n",
    "        If an X has not been seen before, will return 0.0.\n",
    "        '''\n",
    "        return np.array([self.values_map.get(x, 0.) for x in x_values_seq])\n",
    "\n",
    "    def update(self, xy_vals_seq: Iterable[Tuple[X, float]]) -> Tabular[X]:\n",
    "        '''Update the approximation with the given points.\n",
    "\n",
    "        Each X keeps a count n of how many times it was updated, and\n",
    "        each subsequent update is discounted by\n",
    "        count_to_weight_func(n), which defines our learning rate.\n",
    "\n",
    "        '''\n",
    "        values_map: Dict[X, float] = dict(self.values_map)\n",
    "        counts_map: Dict[X, int] = dict(self.counts_map)\n",
    "\n",
    "        for x, y in xy_vals_seq:\n",
    "            counts_map[x] = counts_map.get(x, 0) + 1\n",
    "            weight: float = self.count_to_weight_func(counts_map.get(x, 0))\n",
    "            values_map[x] = weight * y + (1 - weight) * values_map.get(x, 0.)\n",
    "\n",
    "        return replace(\n",
    "            self,\n",
    "            values_map=values_map,\n",
    "            counts_map=counts_map\n",
    "        )\n",
    "\n",
    "    def solve(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]],\n",
    "        error_tolerance: Optional[float] = None\n",
    "    ) -> Tabular[X]:\n",
    "        values_map: Dict[X, float] = {}\n",
    "        counts_map: Dict[X, int] = {}\n",
    "        for x, y in xy_vals_seq:\n",
    "            counts_map[x] = counts_map.get(x, 0) + 1\n",
    "            weight: float = self.count_to_weight_func(counts_map.get(x, 0))\n",
    "            values_map[x] = weight * y + (1 - weight) * values_map.get(x, 0.)\n",
    "        return replace(\n",
    "            self,\n",
    "            values_map=values_map,\n",
    "            counts_map=counts_map\n",
    "        )\n",
    "\n",
    "    def within(self, other: FunctionApprox[X], tolerance: float) -> bool:\n",
    "        if isinstance(other, Tabular):\n",
    "            return\\\n",
    "                all(abs(self.values_map[s] - other.values_map[s]) <= tolerance\n",
    "                    for s in self.values_map)\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BSplineApprox(FunctionApprox[X]):\n",
    "    feature_function: Callable[[X], float]\n",
    "    degree: int\n",
    "    knots: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    coeffs: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "\n",
    "    def get_feature_values(self, x_values_seq: Iterable[X]) -> Sequence[float]:\n",
    "        return [self.feature_function(x) for x in x_values_seq]\n",
    "\n",
    "    def representational_gradient(self, x_value: X) -> BSplineApprox[X]:\n",
    "        feature_val: float = self.feature_function(x_value)\n",
    "        eps: float = 1e-6\n",
    "        one_hots: np.array = np.eye(len(self.coeffs))\n",
    "        return replace(\n",
    "            self,\n",
    "            coeffs=np.array([(\n",
    "                BSpline(\n",
    "                    self.knots,\n",
    "                    c + one_hots[i] * eps,\n",
    "                    self.degree\n",
    "                )(feature_val) -\n",
    "                BSpline(\n",
    "                    self.knots,\n",
    "                    c - one_hots[i] * eps,\n",
    "                    self.degree\n",
    "                )(feature_val)\n",
    "            ) / (2 * eps) for i, c in enumerate(self.coeffs)]))\n",
    "\n",
    "    def evaluate(self, x_values_seq: Iterable[X]) -> np.ndarray:\n",
    "        spline_func: Callable[[Sequence[float]], np.ndarray] = \\\n",
    "            BSpline(self.knots, self.coeffs, self.degree)\n",
    "        return spline_func(self.get_feature_values(x_values_seq))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]]\n",
    "    ) -> BSplineApprox[X]:\n",
    "        x_vals, y_vals = zip(*xy_vals_seq)\n",
    "        feature_vals: Sequence[float] = self.get_feature_values(x_vals)\n",
    "        sorted_pairs: Sequence[Tuple[float, float]] = \\\n",
    "            sorted(zip(feature_vals, y_vals), key=itemgetter(0))\n",
    "        new_knots, new_coeffs, _ = splrep(\n",
    "            [f for f, _ in sorted_pairs],\n",
    "            [y for _, y in sorted_pairs],\n",
    "            k=self.degree\n",
    "        )\n",
    "        return replace(\n",
    "            self,\n",
    "            knots=new_knots,\n",
    "            coeffs=new_coeffs\n",
    "        )\n",
    "\n",
    "    def solve(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]],\n",
    "        error_tolerance: Optional[float] = None\n",
    "    ) -> BSplineApprox[X]:\n",
    "        return self.update(xy_vals_seq)\n",
    "\n",
    "    def within(self, other: FunctionApprox[X], tolerance: float) -> bool:\n",
    "        if isinstance(other, BSplineApprox):\n",
    "            return \\\n",
    "                np.all(np.abs(self.knots - other.knots) <= tolerance).item() \\\n",
    "                and \\\n",
    "                np.all(np.abs(self.coeffs - other.coeffs) <= tolerance).item()\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class AdamGradient:\n",
    "    learning_rate: float\n",
    "    decay1: float\n",
    "    decay2: float\n",
    "\n",
    "    @staticmethod\n",
    "    def default_settings() -> AdamGradient:\n",
    "        return AdamGradient(\n",
    "            learning_rate=0.001,\n",
    "            decay1=0.9,\n",
    "            decay2=0.999\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Weights:\n",
    "    adam_gradient: AdamGradient\n",
    "    time: int\n",
    "    weights: np.ndarray\n",
    "    adam_cache1: np.ndarray\n",
    "    adam_cache2: np.ndarray\n",
    "\n",
    "    @staticmethod\n",
    "    def create(\n",
    "        weights: np.ndarray,\n",
    "        adam_gradient: AdamGradient = AdamGradient.default_settings(),\n",
    "        adam_cache1: Optional[np.ndarray] = None,\n",
    "        adam_cache2: Optional[np.ndarray] = None\n",
    "    ) -> Weights:\n",
    "        return Weights(\n",
    "            adam_gradient=adam_gradient,\n",
    "            time=0,\n",
    "            weights=weights,\n",
    "            adam_cache1=np.zeros_like(\n",
    "                weights\n",
    "            ) if adam_cache1 is None else adam_cache1,\n",
    "            adam_cache2=np.zeros_like(\n",
    "                weights\n",
    "            ) if adam_cache2 is None else adam_cache2\n",
    "        )\n",
    "\n",
    "    def update(self, gradient: np.ndarray) -> Weights:\n",
    "        time: int = self.time + 1\n",
    "        new_adam_cache1: np.ndarray = self.adam_gradient.decay1 * \\\n",
    "            self.adam_cache1 + (1 - self.adam_gradient.decay1) * gradient\n",
    "        new_adam_cache2: np.ndarray = self.adam_gradient.decay2 * \\\n",
    "            self.adam_cache2 + (1 - self.adam_gradient.decay2) * gradient ** 2\n",
    "        corrected_m: np.ndarray = new_adam_cache1 / \\\n",
    "            (1 - self.adam_gradient.decay1 ** time)\n",
    "        corrected_v: np.ndarray = new_adam_cache2 / \\\n",
    "            (1 - self.adam_gradient.decay2 ** time)\n",
    "\n",
    "        new_weights: np.ndarray = self.weights - \\\n",
    "            self.adam_gradient.learning_rate * corrected_m / \\\n",
    "            (np.sqrt(corrected_v) + SMALL_NUM)\n",
    "\n",
    "        return replace(\n",
    "            self,\n",
    "            time=time,\n",
    "            weights=new_weights,\n",
    "            adam_cache1=new_adam_cache1,\n",
    "            adam_cache2=new_adam_cache2,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-4-26ae0fef873d>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-26ae0fef873d>\"\u001b[1;36m, line \u001b[1;32m26\u001b[0m\n\u001b[1;33m    def lr_func(n: int) -> float:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "    def solve(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]],\n",
    "        error_tolerance: Optional[float] = None\n",
    "    ) -> DNNApprox[X]:\n",
    "        tol: float = 1e-6 if error_tolerance is None else error_tolerance\n",
    "\n",
    "        def done(\n",
    "            a: DNNApprox[X],\n",
    "            b: DNNApprox[X],\n",
    "            tol: float = tol\n",
    "        ) -> bool:\n",
    "            return a.within(b, tol)\n",
    "\n",
    "        return iterate.converged(\n",
    "            self.iterate_updates(itertools.repeat(xy_vals_seq)),\n",
    "            done=done\n",
    "        )\n",
    "\n",
    "\n",
    "def learning_rate_schedule(\n",
    "    initial_learning_rate: float,\n",
    "    half_life: float,\n",
    "    exponent: float\n",
    ") -> Callable[[int], float]:\n",
    "    def lr_func(n: int) -> float:\n",
    "        return initial_learning_rate * (1 + (n - 1) / half_life) ** -exponent\n",
    "    return lr_func\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    from scipy.stats import norm\n",
    "    from pprint import pprint\n",
    "\n",
    "    alpha = 2.0\n",
    "    beta_1 = 10.0\n",
    "    beta_2 = 4.0\n",
    "    beta_3 = -6.0\n",
    "    beta = (beta_1, beta_2, beta_3)\n",
    "\n",
    "    x_pts = np.arange(-10.0, 10.5, 0.5)\n",
    "    y_pts = np.arange(-10.0, 10.5, 0.5)\n",
    "    z_pts = np.arange(-10.0, 10.5, 0.5)\n",
    "    pts: Sequence[Tuple[float, float, float]] = \\\n",
    "        [(x, y, z) for x in x_pts for y in y_pts for z in z_pts]\n",
    "\n",
    "    def superv_func(pt):\n",
    "        return alpha + np.dot(beta, pt)\n",
    "\n",
    "    n = norm(loc=0., scale=2.)\n",
    "    xy_vals_seq: Sequence[Tuple[Tuple[float, float, float], float]] = \\\n",
    "        [(x, superv_func(x) + n.rvs(size=1)[0]) for x in pts]\n",
    "\n",
    "    ag = AdamGradient(\n",
    "        learning_rate=0.5,\n",
    "        decay1=0.9,\n",
    "        decay2=0.999\n",
    "    )\n",
    "    ffs = [\n",
    "        lambda _: 1.,\n",
    "        lambda x: x[0],\n",
    "        lambda x: x[1],\n",
    "        lambda x: x[2]\n",
    "    ]\n",
    "\n",
    "    lfa = LinearFunctionApprox.create(\n",
    "         feature_functions=ffs,\n",
    "         adam_gradient=ag,\n",
    "         regularization_coeff=0.001,\n",
    "         direct_solve=True\n",
    "    )\n",
    "\n",
    "    lfa_ds = lfa.solve(xy_vals_seq)\n",
    "    print(\"Direct Solve\")\n",
    "    pprint(lfa_ds.weights)\n",
    "    errors: np.ndarray = lfa_ds.evaluate(pts) - \\\n",
    "        np.array([y for _, y in xy_vals_seq])\n",
    "    print(\"Mean Squared Error\")\n",
    "    pprint(np.mean(errors * errors))\n",
    "    print()\n",
    "\n",
    "    print(\"Linear Gradient Solve\")\n",
    "    for _ in range(100):\n",
    "        print(\"Weights\")\n",
    "        pprint(lfa.weights)\n",
    "        errors: np.ndarray = lfa.evaluate(pts) - \\\n",
    "            np.array([y for _, y in xy_vals_seq])\n",
    "        print(\"Mean Squared Error\")\n",
    "        pprint(np.mean(errors * errors))\n",
    "        lfa = lfa.update(xy_vals_seq)\n",
    "        print()\n",
    "\n",
    "    ds = DNNSpec(\n",
    "        neurons=[2],\n",
    "        bias=True,\n",
    "        hidden_activation=lambda x: x,\n",
    "        hidden_activation_deriv=lambda x: np.ones_like(x),\n",
    "        output_activation=lambda x: x,\n",
    "        output_activation_deriv=lambda x: np.ones_like(x)\n",
    "    )\n",
    "\n",
    "    dnna = DNNApprox.create(\n",
    "        feature_functions=ffs,\n",
    "        dnn_spec=ds,\n",
    "        adam_gradient=ag,\n",
    "        regularization_coeff=0.01\n",
    "    )\n",
    "    print(\"DNN Gradient Solve\")\n",
    "    for _ in range(100):\n",
    "        print(\"Weights\")\n",
    "        pprint(dnna.weights)\n",
    "        errors: np.ndarray = dnna.evaluate(pts) - \\\n",
    "            np.array([y for _, y in xy_vals_seq])\n",
    "        print(\"Mean Squared Error\")\n",
    "        pprint(np.mean(errors * errors))\n",
    "        dnna = dnna.update(xy_vals_seq)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
