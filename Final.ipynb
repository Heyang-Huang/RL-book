{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heyang Huang heyangh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$G = \\{(i,j)|0 \\leq i < m, 0\\leq j < n\\}$\n",
    "<br>$B=\\{(i,j)|(i,j)\\in G,(i,j)\\in \\text{uninhabitable cells known as\n",
    "\"blocks\"}\\}$\n",
    "<br>\n",
    "<br>$S=G-B=\\{(i,j)|(i,j)\\in G,(i,j)\\notin B\\}$ \n",
    "<br>$T=\\{(i,j)|(i,j)\\in S,(i,j)\\in \\text{goal cells}\\}$ \n",
    "<br>$N=S-T=G-B-T=\\{(i,j)|(i,j)\\in S,(i,j)\\notin T \\}$ \n",
    "<br>$A = \\{(0,-1), (0,1), (-1,0), (1,0)\\}$\n",
    "<br>Discount factor: $\\gamma=1$\n",
    "<br>\n",
    "$Pr(s,a,s',r)=Pr((x,y),(a_i,a_j),(x',y'),r)=$\n",
    "<br>\n",
    "$\n",
    "\\begin{cases}\n",
    "        \\\\\n",
    "        1 & \\text{if }r=-1,x^*=x',y^*=y',(x^*,y^*)\\in T\\\\\n",
    "        \\\\\n",
    "        1-p_{1,x^*}-p_{2,x^*} & \\text{if }r=-1, x^*=x',y^*=y',(x^*,y^*)\\in N,(x^*+1,y^*)\\in S,(x^*-1,y^*)\\in S\\\\\n",
    "        p_{1,x^*} & \\text{if } r=-1,x^*-1=x',y^*=y',(x^*,y^*)\\in N,(x^*+1,y^*)\\in S,(x^*-1,y^*)\\in S\\\\\n",
    "        p_{2,x^*} & \\text{if } r=-1,x^*+1=x',y^*=y',(x^*,y^*)\\in N,(x^*+1,y^*)\\in S,(x^*-1,y^*)\\in S\\\\\n",
    "        \\\\\n",
    "        1-p_{1,x^*}-p_{2,x^*} & \\text{if } r=-1,x^*=x',y^*=y',(x^*,y^*)\\in N,(x^*+1,y^*)\\in S,(x^*-1,y^*)\\notin S\\\\\n",
    "        p_{1,x^*} & \\text{if } r=-(1+b),x^*=x',y^*=y',(x^*,y^*)\\in N,(x^*+1,y^*)\\in S,(x^*-1,y^*)\\notin S\\\\\n",
    "        p_{2,x^*} & \\text{if } r=-1,x^*+1=x',y^*=y',(x^*,y^*)\\in N,(x^*+1,y^*)\\in S,(x^*-1,y^*)\\notin S\\\\\n",
    "        \\\\\n",
    "        1-p_{1,x^*}-p_{2,x^*} & \\text{if } r=-1,x^*=x',y^*=y',(x^*,y^*)\\in N,(x^*+1,y^*)\\notin S,(x^*-1,y^*)\\in S\\\\\n",
    "        p_{2,x^*} & \\text{if } r=-(1+b),x^*=x',y^*=y',(x^*,y^*)\\in N,(x^*+1,y^*)\\notin S,(x^*-1,y^*)\\in S\\\\\n",
    "        p_{1,x^*} & \\text{if } r=-1,x^*-1=x',y^*=y',(x^*,y^*)\\in N,(x^*+1,y^*)\\notin S,(x^*-1,y^*)\\in S\\\\\n",
    "        \\\\\n",
    "        1-p_{1,x^*}-p_{2,x^*} & \\text{if } r=-1,x^*=x',y^*=y',(x^*,y^*)\\in N,(x^*+1,y^*)\\notin S,(x^*-1,y^*)\\notin S\\\\\n",
    "        p_{1,x^*}+p_{2,x^*} & \\text{if } r=-(1+b),x^*=x',y^*=y',(x^*,y^*)\\in N,(x^*+1,y^*)\\notin S,(x^*-1,y^*)\\notin S\\\\\n",
    "        \\\\\n",
    "        0 & \\text{OTW} \\\\\n",
    "        \\end{cases} \n",
    "$\n",
    "$(\\text{ Let } x^*=x+a_i \\text{ and } y^*=y+a_j \\text{ for conciseness of notation})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Sequence, Set, Mapping, Dict, Callable, Optional\n",
    "from dataclasses import dataclass\n",
    "from operator import itemgetter\n",
    "from rl.distribution import Categorical, Choose, Constant\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import StateActionMapping\n",
    "from rl.markov_decision_process import FinitePolicy\n",
    "from rl.dynamic_programming import value_iteration_result, V\n",
    "\n",
    "'''\n",
    "Cell specifies (row, column) coordinate\n",
    "'''\n",
    "Cell = Tuple[int, int]\n",
    "CellSet = Set[Cell]\n",
    "Move = Tuple[int, int]\n",
    "'''\n",
    "WindSpec specifies a random vectical wind for each column.\n",
    "Each random vertical wind is specified by a (p1, p2) pair\n",
    "where p1 specifies probability of Downward Wind (could take you\n",
    "one step lower in row coordinate unless prevented by a block or\n",
    "boundary) and p2 specifies probability of Upward Wind (could take\n",
    "you onw step higher in column coordinate unless prevented by a\n",
    "block or boundary). If one bumps against a block or boundary, one\n",
    "incurs a bump cost and doesn't move. The remaining probability\n",
    "1- p1 - p2 corresponds to No Wind.\n",
    "'''\n",
    "WindSpec = Sequence[Tuple[float, float]]\n",
    "\n",
    "possible_moves: Mapping[Move, str] = {\n",
    "    (-1, 0): 'D',\n",
    "    (1, 0): 'U',\n",
    "    (0, -1): 'L',\n",
    "    (0, 1): 'R'\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class WindyGrid:\n",
    "\n",
    "    rows: int  # number of grid rows\n",
    "    columns: int  # number of grid columns\n",
    "    blocks: CellSet  # coordinates of block cells\n",
    "    terminals: CellSet  # coordinates of goal cells\n",
    "    wind: WindSpec  # spec of vertical random wind for the columns\n",
    "    bump_cost: float  # cost of bumping against block or boundary\n",
    "\n",
    "    def validate_spec(self) -> bool:\n",
    "        b1 = self.rows >= 2\n",
    "        b2 = self.columns >= 2\n",
    "        b3 = all(0 <= r < self.rows and 0 <= c < self.columns\n",
    "                 for r, c in self.blocks)\n",
    "        b4 = len(self.terminals) >= 1\n",
    "        b5 = all(0 <= r < self.rows and 0 <= c < self.columns and\n",
    "                 (r, c) not in self.blocks for r, c in self.terminals)\n",
    "        b6 = len(self.wind) == self.columns\n",
    "        b7 = all(0. <= p1 <= 1. and 0. <= p2 <= 1. and p1 + p2 <= 1.\n",
    "                 for p1, p2 in self.wind)\n",
    "        b8 = self.bump_cost > 0.\n",
    "        return all([b1, b2, b3, b4, b5, b6, b7, b8])\n",
    "\n",
    "    def print_wind_and_bumps(self) -> None:\n",
    "        for i, (d, u) in enumerate(self.wind):\n",
    "            print(f\"Column {i:d}: Down Prob = {d:.2f}, Up Prob = {u:.2f}\")\n",
    "        print(f\"Bump Cost = {self.bump_cost:.2f}\")\n",
    "        print()\n",
    "\n",
    "    @staticmethod\n",
    "    def add_tuples(a: Cell, b: Cell) -> Cell:\n",
    "        return a[0] + b[0], a[1] + b[1]\n",
    "\n",
    "    def is_valid_state(self, cell: Cell) -> bool:\n",
    "        '''\n",
    "        checks if a cell is a valid state of the MDP\n",
    "        '''\n",
    "        return 0 <= cell[0] < self.rows and 0 <= cell[1] < self.columns \\\n",
    "            and cell not in self.blocks\n",
    "\n",
    "    def get_all_nt_states(self) -> CellSet:\n",
    "        '''\n",
    "        returns all the non-terminal states\n",
    "        '''\n",
    "        return {(i, j) for i in range(self.rows) for j in range(self.columns)\n",
    "                if (i, j) not in set.union(self.blocks, self.terminals)}\n",
    "\n",
    "    def get_actions_and_next_states(self, nt_state: Cell) \\\n",
    "            -> Set[Tuple[Move, Cell]]:\n",
    "        '''\n",
    "        given a non-terminal state, returns the set of all possible\n",
    "        (action, next_state) pairs\n",
    "        '''\n",
    "        temp: Set[Tuple[Move, Cell]] = {(a, WindyGrid.add_tuples(nt_state, a))\n",
    "                                        for a in possible_moves}\n",
    "        return {(a, s) for a, s in temp if self.is_valid_state(s)}\n",
    "\n",
    "    def get_transition_probabilities(self, nt_state: Cell) \\\n",
    "            -> Mapping[Move, Categorical[Tuple[Cell, float]]]:\n",
    "        '''\n",
    "        given a non-terminal state, return a dictionary whose\n",
    "        keys are the valid actions (moves) from the given state\n",
    "        and the corresponding values are the associated probabilities\n",
    "        (following that move) of the (next_state, reward) pairs.\n",
    "        The probabilities are determined from the wind probabilities\n",
    "        of the column one is in after the move. Note that if one moves\n",
    "        to a goal cell (terminal state), then one ends up in that\n",
    "        goal cell with 100% probability (i.e., no wind exposure in a\n",
    "        goal cell).\n",
    "        '''\n",
    "        d: Dict[Move, Categorical[Tuple[Cell, float]]] = {}\n",
    "        for a, (r, c) in self.get_actions_and_next_states(nt_state):\n",
    "            if (r, c) in self.terminals:\n",
    "                d[a] = Categorical({((r, c), -1.): 1.})\n",
    "            else:\n",
    "                d1={}\n",
    "                up_valid=self.is_valid_state((r+1,c))\n",
    "                down_valid=self.is_valid_state((r-1,c))\n",
    "                #both up and down valid#\n",
    "                if up_valid and down_valid:\n",
    "                    d1[((r,c),-1)]=1-self.wind[c][0]-self.wind[c][1]\n",
    "                    d1[((r-1,c),-1)]=self.wind[c][0]\n",
    "                    d1[((r+1,c),-1)]=self.wind[c][1]\n",
    "                #only up valid#\n",
    "                elif up_valid:\n",
    "                    d1[((r,c),-1)]=1-self.wind[c][0]-self.wind[c][1]\n",
    "                    d1[((r,c),-1-self.bump_cost)]=self.wind[c][0]\n",
    "                    d1[((r+1,c),-1)]=self.wind[c][1]\n",
    "                #only down valid#\n",
    "                elif down_valid:\n",
    "                    d1[((r,c),-1)]=1-self.wind[c][0]-self.wind[c][1]\n",
    "                    d1[((r,c),-1-self.bump_cost)]=self.wind[c][1]\n",
    "                    d1[((r-1,c),-1)]=self.wind[c][0]\n",
    "                #neither up nor down valid#\n",
    "                else:\n",
    "                    d1[((r,c),-1)]=1-self.wind[c][0]-self.wind[c][1]\n",
    "                    d1[((r,c),-1-self.bump_cost)]=self.wind[c][0]+self.wind[c][1]\n",
    "                d[a]=Categorical(d1)\n",
    "        return d\n",
    "\n",
    "    def get_finite_mdp(self) -> FiniteMarkovDecisionProcess[Cell, Move]:\n",
    "        '''\n",
    "        returns the FiniteMarkovDecision object for this windy grid problem\n",
    "        '''\n",
    "        d1: StateActionMapping[Cell, Move] = \\\n",
    "            {s: self.get_transition_probabilities(s) for s in\n",
    "             self.get_all_nt_states()}\n",
    "        d2: StateActionMapping[Cell, Move] = {s: None for s in self.terminals}\n",
    "        return FiniteMarkovDecisionProcess({**d1, **d2})\n",
    "\n",
    "    def get_vi_vf_and_policy(self) -> Tuple[V[Cell], FinitePolicy[Cell, Move]]:\n",
    "        '''\n",
    "        Performs the Value Iteration DP algorithm returning the\n",
    "        Optimal Value Function (as a V[Cell]) and the Optimal Policy\n",
    "        (as a FinitePolicy[Cell, Move])\n",
    "        '''\n",
    "        return value_iteration_result(self.get_finite_mdp(), gamma=1.)\n",
    "\n",
    "    @staticmethod\n",
    "    def epsilon_greedy_action(\n",
    "        nt_state: Cell,\n",
    "        q: Mapping[Cell, Mapping[Move, float]],\n",
    "        epsilon: float\n",
    "    ) -> Move:\n",
    "        '''\n",
    "        given a non-terminal state, a Q-Value Function (in the form of a\n",
    "        {state: {action: Expected Return}} dictionary) and epislon, return\n",
    "        an action sampled from the probability distribution implied by an\n",
    "        epsilon-greedy policy that is derived from the Q-Value Function.\n",
    "        '''\n",
    "        action_values: Mapping[Move, float] = q[nt_state]\n",
    "        greedy_action: Move = max(action_values.items(), key=itemgetter(1))[0]\n",
    "        return Categorical(\n",
    "            {a: epsilon / len(action_values) +\n",
    "             (1 - epsilon if a == greedy_action else 0.)\n",
    "             for a in action_values}\n",
    "        ).sample()\n",
    "\n",
    "    def get_states_actions_dict(self) -> Mapping[Cell, Optional[Set[Move]]]:\n",
    "        '''\n",
    "        Returns a dictionary whose keys are the states and the corresponding\n",
    "        values are the set of actions for the state (if the key is a\n",
    "        non-terminal state) or is None if the state is a terminal state.\n",
    "        '''\n",
    "        d1: Mapping[Cell, Optional[Set[Move]]] = \\\n",
    "            {s: {a for a, _ in self.get_actions_and_next_states(s)}\n",
    "             for s in self.get_all_nt_states()}\n",
    "        d2: Mapping[Cell, Optional[Set[Move]]] = \\\n",
    "            {s: None for s in self.terminals}\n",
    "        return {**d1, **d2}\n",
    "\n",
    "    def get_sarsa_vf_and_policy(\n",
    "        self,\n",
    "        states_actions_dict: Mapping[Cell, Optional[Set[Move]]],\n",
    "        sample_func: Callable[[Cell, Move], Tuple[Cell, float]],\n",
    "        episodes: int = 10000,\n",
    "        step_size: float = 0.01\n",
    "    ) -> Tuple[V[Cell], FinitePolicy[Cell, Move]]:\n",
    "        '''\n",
    "        states_actions_dict gives us the set of possible moves from\n",
    "        a non-block cell.\n",
    "        sample_func is a function with two inputs: state and action,\n",
    "        and with output as a sampled pair of (next_state, reward).\n",
    "        '''\n",
    "        q: Dict[Cell, Dict[Move, float]] = \\\n",
    "            {s: {a: 0. for a in actions} for s, actions in\n",
    "             states_actions_dict.items() if actions is not None}\n",
    "        nt_states: CellSet = {s for s in q}\n",
    "        uniform_states: Choose[Cell] = Choose(nt_states)\n",
    "        for episode_num in range(episodes):\n",
    "            epsilon: float = 1.0 / (episode_num + 1)\n",
    "            state: Cell = uniform_states.sample()\n",
    "            '''\n",
    "            write your code here\n",
    "            update the dictionary q initialized above according\n",
    "            to the SARSA algorithm's Q-Value Function updates.\n",
    "            '''\n",
    "            # Since we are at a non-terminating state, we can always get an action wrt epsilon greedy policy\n",
    "            sampled_a=self.epsilon_greedy_action(nt_state=state,q=q,epsilon=epsilon)\n",
    "            while True:\n",
    "                # sample the next state and reward pair\n",
    "                nxt_state,r=sample_func(state,sampled_a)\n",
    "                # if next state is T,we update with Q(s_t+1,A_t+1)=0 and stop this episode\n",
    "                if nxt_state in self.terminals:\n",
    "                    q[state][sampled_a]=q[state][sampled_a]+step_size*(r-q[state][sampled_a])\n",
    "                    break\n",
    "                # Else update q table and continue\n",
    "                sampled_nxt_a=self.epsilon_greedy_action(nt_state=nxt_state,q=q,epsilon=epsilon)\n",
    "                q[state][sampled_a]=q[state][sampled_a]+step_size*(r+q[nxt_state][sampled_nxt_a]-q[state][sampled_a])\n",
    "                state=nxt_state\n",
    "                sampled_a=sampled_nxt_a\n",
    "        vf_dict: V[Cell] = {s: max(d.values()) for s, d in q.items()}\n",
    "        policy: FinitePolicy[Cell, Move] = FinitePolicy(\n",
    "            {s: Constant(max(d.items(), key=itemgetter(1))[0])\n",
    "             for s, d in q.items()}\n",
    "        )\n",
    "        return (vf_dict, policy)\n",
    "\n",
    "    def get_q_learning_vf_and_policy(\n",
    "        self,\n",
    "        states_actions_dict: Mapping[Cell, Optional[Set[Move]]],\n",
    "        sample_func: Callable[[Cell, Move], Tuple[Cell, float]],\n",
    "        episodes: int = 10000,\n",
    "        step_size: float = 0.01,\n",
    "        epsilon: float = 0.1\n",
    "    ) -> Tuple[V[Cell], FinitePolicy[Cell, Move]]:\n",
    "        '''\n",
    "        states_actions_dict gives us the set of possible moves from\n",
    "        a non-block cell.\n",
    "        sample_func is a function with two inputs: state and action,\n",
    "        and with output as a sampled pair of (next_state, reward).\n",
    "        '''\n",
    "        q: Dict[Cell, Dict[Move, float]] = \\\n",
    "            {s: {a: 0. for a in actions} for s, actions in\n",
    "             states_actions_dict.items() if actions is not None}\n",
    "        nt_states: CellSet = {s for s in q}\n",
    "        uniform_states: Choose[Cell] = Choose(nt_states)\n",
    "        for episode_num in range(episodes):\n",
    "            state: Cell = uniform_states.sample()\n",
    "            '''\n",
    "            write your code here\n",
    "            update the dictionary q initialized above according\n",
    "            to the Q-learning algorithm's Q-Value Function updates.\n",
    "            '''\n",
    "        # Since we are at a non-terminating state, we can always get an action wrt epsilon greedy policy\n",
    "            sampled_a=self.epsilon_greedy_action(nt_state=state,q=q,epsilon=epsilon)\n",
    "            while True:\n",
    "                # sample the next state and reward pair\n",
    "                nxt_state,r=sample_func(state,sampled_a)\n",
    "                # if next state is T,we update with Q(s_t+1,A_t+1)=0 and stop this episode\n",
    "                if nxt_state in self.terminals:\n",
    "                    q[state][sampled_a]=q[state][sampled_a]+step_size*(r-q[state][sampled_a])\n",
    "                    break\n",
    "                # Else we take next action from a greedy policy(epsilon=0) and update Q table\n",
    "                sampled_nxt_a=self.epsilon_greedy_action(nt_state=nxt_state,q=q,epsilon=0)\n",
    "                q[state][sampled_a]=q[state][sampled_a]+step_size*(r+q[nxt_state][sampled_nxt_a]-q[state][sampled_a])\n",
    "                state=nxt_state\n",
    "                sampled_a=sampled_nxt_a   \n",
    "            \n",
    "        vf_dict: V[Cell] = {s: max(d.values()) for s, d in q.items()}\n",
    "        policy: FinitePolicy[Cell, Move] = FinitePolicy(\n",
    "            {s: Constant(max(d.items(), key=itemgetter(1))[0])\n",
    "             for s, d in q.items()}\n",
    "        )\n",
    "        return (vf_dict, policy)\n",
    "\n",
    "    def print_vf_and_policy(\n",
    "        self,\n",
    "        vf_dict: V[Cell],\n",
    "        policy: FinitePolicy[Cell, Move]\n",
    "    ) -> None:\n",
    "        display = \"%5.2f\"\n",
    "        display1 = \"%5d\"\n",
    "        vf_full_dict = {\n",
    "            **{s: display % -v for s, v in vf_dict.items()},\n",
    "            **{s: display % 0.0 for s in self.terminals},\n",
    "            **{s: 'X' * 5 for s in self.blocks}\n",
    "        }\n",
    "        print(\"   \" + \" \".join([display1 % j for j in range(self.columns)]))\n",
    "        for i in range(self.rows - 1, -1, -1):\n",
    "            print(\"%2d \" % i + \" \".join(vf_full_dict[(i, j)]\n",
    "                                        for j in range(self.columns)))\n",
    "        print()\n",
    "        pol_full_dict = {\n",
    "            **{s: possible_moves[policy.act(s).value]\n",
    "               for s in self.get_all_nt_states()},\n",
    "            **{s: 'T' for s in self.terminals},\n",
    "            **{s: 'X' for s in self.blocks}\n",
    "        }\n",
    "        print(\"   \" + \" \".join([\"%2d\" % j for j in range(self.columns)]))\n",
    "        for i in range(self.rows - 1, -1, -1):\n",
    "            print(\"%2d  \" % i + \"  \".join(pol_full_dict[(i, j)]\n",
    "                                          for j in range(self.columns)))\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 0: Down Prob = 0.00, Up Prob = 0.90\n",
      "Column 1: Down Prob = 0.00, Up Prob = 0.80\n",
      "Column 2: Down Prob = 0.70, Up Prob = 0.00\n",
      "Column 3: Down Prob = 0.80, Up Prob = 0.00\n",
      "Column 4: Down Prob = 0.90, Up Prob = 0.00\n",
      "Bump Cost = 4.00\n",
      "\n",
      "Number of Iterations: 111\n",
      "Value Iteration\n",
      "\n",
      "       0     1     2     3     4\n",
      " 4 XXXXX  5.25  2.02  1.10  1.00\n",
      " 3 XXXXX  8.53  5.20  1.00  0.00\n",
      " 2  9.21  6.90  8.53 XXXXX  1.00\n",
      " 1  8.36  9.21  8.36 12.16 11.00\n",
      " 0 10.12 XXXXX XXXXX 17.16 XXXXX\n",
      "\n",
      "    0  1  2  3  4\n",
      " 4  X  R  R  R  D\n",
      " 3  X  R  R  R  T\n",
      " 2  R  U  U  X  U\n",
      " 1  R  U  L  L  U\n",
      " 0  U  X  X  U  X\n",
      "\n",
      "SARSA\n",
      "\n",
      "       0     1     2     3     4\n",
      " 4 XXXXX  5.32  2.02  1.11  1.00\n",
      " 3 XXXXX  8.25  5.28  1.00  0.00\n",
      " 2  9.20  6.82  8.16 XXXXX  1.00\n",
      " 1  8.26  9.17  8.37 11.87 11.68\n",
      " 0 10.11 XXXXX XXXXX 17.05 XXXXX\n",
      "\n",
      "    0  1  2  3  4\n",
      " 4  X  R  R  R  D\n",
      " 3  X  R  R  R  T\n",
      " 2  R  U  U  X  U\n",
      " 1  R  U  L  L  U\n",
      " 0  U  X  X  U  X\n",
      "\n",
      "Q-Learning\n",
      "\n",
      "       0     1     2     3     4\n",
      " 4 XXXXX  5.34  2.02  1.08  1.00\n",
      " 3 XXXXX  8.45  5.17  1.00  0.00\n",
      " 2  9.24  6.84  8.59 XXXXX  1.00\n",
      " 1  8.41  9.27  8.32 12.11 11.58\n",
      " 0 10.18 XXXXX XXXXX 17.35 XXXXX\n",
      "\n",
      "    0  1  2  3  4\n",
      " 4  X  R  R  R  D\n",
      " 3  X  R  R  R  T\n",
      " 2  R  U  U  X  U\n",
      " 1  R  U  L  L  U\n",
      " 0  U  X  X  U  X\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    wg = WindyGrid(\n",
    "        rows=5,\n",
    "        columns=5,\n",
    "        blocks={(0, 1), (0, 2), (0, 4), (2, 3), (3, 0), (4, 0)},\n",
    "        terminals={(3, 4)},\n",
    "        wind=[(0., 0.9), (0.0, 0.8), (0.7, 0.0), (0.8, 0.0), (0.9, 0.0)],\n",
    "        bump_cost=4.0\n",
    "    )\n",
    "    valid = wg.validate_spec()\n",
    "    if valid:\n",
    "        wg.print_wind_and_bumps()\n",
    "        vi_vf_dict, vi_policy = wg.get_vi_vf_and_policy()\n",
    "        print(\"Value Iteration\\n\")\n",
    "        wg.print_vf_and_policy(\n",
    "            vf_dict=vi_vf_dict,\n",
    "            policy=vi_policy\n",
    "        )\n",
    "        mdp: FiniteMarkovDecisionProcess[Cell, Move] = wg.get_finite_mdp()\n",
    "\n",
    "        def sample_func(state: Cell, action: Move) -> Tuple[Cell, float]:\n",
    "            return mdp.step(state, action).sample()\n",
    "\n",
    "        sarsa_vf_dict, sarsa_policy = wg.get_sarsa_vf_and_policy(\n",
    "            states_actions_dict=wg.get_states_actions_dict(),\n",
    "            sample_func=sample_func,\n",
    "            episodes=100000,\n",
    "            step_size=0.01\n",
    "        )\n",
    "        print(\"SARSA\\n\")\n",
    "        wg.print_vf_and_policy(\n",
    "            vf_dict=sarsa_vf_dict,\n",
    "            policy=sarsa_policy\n",
    "        )\n",
    "\n",
    "        ql_vf_dict, ql_policy = wg.get_q_learning_vf_and_policy(\n",
    "            states_actions_dict=wg.get_states_actions_dict(),\n",
    "            sample_func=sample_func,\n",
    "            episodes=30000,\n",
    "            step_size=0.01,\n",
    "            epsilon=0.1\n",
    "        )\n",
    "        print(\"Q-Learning\\n\")\n",
    "        wg.print_vf_and_policy(\n",
    "            vf_dict=ql_vf_dict,\n",
    "            policy=ql_policy\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid Spec of Windy Grid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $c=\\theta x$, we have $x_{t+1}=(1-\\theta)x_t $ or equivalently $x_t=x_0(1-\\theta)^t$\n",
    "<br><br>$V_\\theta(x_0)=E[G_\\theta(x_0)]=\\sum_{t=0}^\\infty \\beta^t U(\\theta x_t)=\\sum_{t=0}^\\infty \\beta^t U(\\theta x_0 (1-\\theta)^t)\n",
    "=\\sum_{t=0}^\\infty \\theta^{1-\\gamma} \\beta^t (1-\\theta)^{t(1-\\gamma)}U(x_0)$\n",
    "<br><br>\n",
    "It's the sum of an infinite geometric series, so we know that $V_\\theta(x_0)={\\theta^{(1-\\gamma)}U(x_0) \\over 1-\\beta(1-\\theta)^{1-\\gamma}}$\n",
    "<br>\n",
    "Therefore,$V_\\theta(x)={\\theta^{(1-\\gamma)}U(x) \\over 1-\\beta{(1-\\theta)}^{1-\\gamma}}={\\theta^{(1-\\gamma)}x^{1-\\gamma} \\over (1-\\beta{(1-\\theta)}^{1-\\gamma})(1-\\gamma)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First order condition: given ${dV \\over d\\theta}=0$, we have:\n",
    "<br>$(1-r)\\theta^{-\\gamma}U(x)(1-\\beta(1-\\theta)^{1-\\gamma})=\\theta^{1-\\gamma}U(x)\\beta (1-\\gamma)(1-\\theta)^{-\\gamma}$\n",
    "<br>$\\theta^*=1-\\beta^{1\\over\\gamma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plug \n",
    "$\\theta^*=1-\\beta^{1\\over\\gamma}$ \n",
    "back into \n",
    "$V_\\theta(x)={\\theta^{(1-\\gamma)}x^{1-\\gamma} \\over (1-\\beta{(1-\\theta)}^{1-\\gamma})(1-\\gamma)}$ \n",
    "to get the maximized $V*(x)$\n",
    "<br> We have \n",
    "$V^*(x)=(1-\\beta^{1\\over\\gamma})^{-\\gamma}{x^{1-\\gamma} \\over 1-\\gamma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> In our example, Bellman Optimality Equation: $V^*(s)=max_{a \\in A}\\{R(s,a)+\\gamma \\sum_{s' \\in N}P(s,a,s')V^*(s')\\} \\forall s \\in N$ can be written as: $V^*(x)=max_{\\theta \\in [0,1]}\\{U(\\theta x)+\\beta V^*((1-\\theta)x)\\} \\forall x=\n",
    "U(\\theta^* x)+\\beta V^*((1-\\theta^*)x)\n",
    "$  \n",
    "<br>\n",
    "plug $\\theta^*=1-\\beta^{1\\over\\gamma}$ and $V^*(x)=(1-\\beta^{1\\over\\gamma})^{-\\gamma}{x^{1-\\gamma} \\over 1-\\gamma}$ back into above equation,we can observe that:\n",
    "<br>\n",
    "$\\beta V^*((1-\\theta^*)x)-V^*(x)={{\\theta^*}^{1-\\gamma}}U(x)=U(\\theta^*x)$\n",
    "<br>therefore, our calculated Optimal Policy and Optimal Value function satisfy the Bellman Optimality Equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S:\\{(b,m,m_{nxt})|b \\in [0,L],m \\in [0,\\infty],m_{nxt}\\in [0,\\infty]\\}$ where $b=$current balance, $m=$current mortgage rate, and $m_{nxt}=$ new rate offered\n",
    "<br>$T:\\{(b,m,m_{nxt})|(b,m,m_{nxt}) \\in S,b=0\\}$\n",
    "<br>$A:\\{1,2\\}$ \n",
    "where 1,2 indicates which mortgage option to choose given observation of current \n",
    "$(b,m,m_{nxt})$\n",
    "<br> Discount Factor:\n",
    "$\\gamma=r/12$ \n",
    "we adjust the annualized discount rate into monthly\n",
    "<br> Assume that the distribution of new offer mortgage rate follows \n",
    "$p(m_{nxt}=m')=f(m')$\n",
    "<br><br>\n",
    "To simplify notation, let \n",
    "$\\delta B(b,m')=P={bm'/12 \\over (1+m'/12)^n-1}$\n",
    "<br>$Pr(s,a,s',r)=Pr((b,m,m_{nxt}),a,(b',m',m_{nxt}'),r)= \\\\\n",
    "\\begin{cases}\n",
    "        f(m_{nxt}') & \\text{if } a=1,m'=m,b'=b-\\delta B(b,m'),r=-{bm' \\over 1-(1+m'/12)^{-n}}\\\\\n",
    "        f(m_{nxt}') & \\text{if } a=2,m'=m_{nxt},b'=b-\\delta B(b,m'),r=-(C+{bm' \\over 1-(1+m'/12)^{-n}})\\\\\n",
    "        0 & \\text{OTW} \\\\\n",
    "        \\end{cases} \n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of MDP formulation: We pause our clock before each month and observe: current loan balance,current mortgage rate, and new rate offered:$(b,m,m_{nxt})$ and make decision whether to accept new loan or not. Since we only care about optimal policy, the loan payment in the first month is considered fixed and independent of our policy, and can be ignored. After we make the decision, everything except for the new rate offered next month(T+2) is determined. In short, this is a discrete time, infinite state space, finite action space MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's identify some traits of our specific MDP problem:\n",
    "<br>1)Though there are terminating states, each episode can last extremely long. Especially when n is large, an agent may choose to keep renewing mortgage loan. \n",
    "<br>2)We have infinite state space and concise action space.\n",
    "\n",
    "<br><br> 1) indicates that TD family algorithms are better than MC.\n",
    "<br> 2) indicates that tabular methods are not suitable and function approximation is superior.\n",
    "\n",
    "<br><br> Therefore, I propose to implement DQN control algorithm. There are several aspects that we need to focus on in our implementation:\n",
    "<br> 1)Batch norm or other normalization method: While \"m\" and binary variable \"a\" tend to vary in a tight range through episodes,b and r can vary from millions to single digits. Therefore, after we retrieve mini-batches from memory, we have to perform some normalization or reward clipping to  raw data to ensure stability.\n",
    "<br> 2)Given n and distribution of M,we may want to consider episodes early-stopping. An agent may never get out of the first few episodes if n is extremely large and our memory space will therefore be flooded with small loan balance atomic experiences.\n",
    "Therefore, we may consider to define a converged threshold and manually terminate the first few episodes.\n",
    "<br >3) In summary, We can simply follow the following DQN procedure:\n",
    "<br> a)Given state (b,m,m_nxt), take action \"a\" according to epsilon-greedy policy extracted from Q-network values Q((b,m,m_nxt), a; w). Remember to choose proper epsilon value since we only have two available action each state.\n",
    "<br>b) Derive the respective next state and reward, store the atmoic experience inreplay memory.\n",
    "<br>c) Sample random mini-batch from replay memory, normalize or clip the batches, and update w with frozen w-\n",
    "<br>d) Infrequently update w- after a reasonable number of time steps.\n",
    "<br>e) Pay attention to the length of each episode and decide whether it's necessary to early stop episodes wrt a thresholld.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
