{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8fa3537fefe1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAssignment2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkov_process\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMarkovRewardProcess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#from ..rl.markov_process import FiniteMarkovRewardProcess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#from ..rl.markov_process import RewardTransition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from ..Assignment2.markov_process import MarkovRewardProcess\n",
    "#from ..rl.markov_process import FiniteMarkovRewardProcess\n",
    "#from ..rl.markov_process import RewardTransition\n",
    "#from ..rl.distribution import SampledDistribution, Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class InventoryState:\n",
    "    on_hand: int\n",
    "    on_order: int\n",
    "\n",
    "    def inventory_position(self) -> int:\n",
    "        return self.on_hand + self.on_order\n",
    "\n",
    "\n",
    "class SimpleInventoryMRP(MarkovRewardProcess[InventoryState]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int,\n",
    "        poisson_lambda: float,\n",
    "        holding_cost: float,\n",
    "        stockout_cost: float\n",
    "    ):\n",
    "        self.capacity = capacity\n",
    "        self.poisson_lambda: float = poisson_lambda\n",
    "        self.holding_cost: float = holding_cost\n",
    "        self.stockout_cost: float = stockout_cost\n",
    "\n",
    "    def transition_reward(\n",
    "        self,\n",
    "        state: InventoryState\n",
    "    ) -> SampledDistribution[Tuple[InventoryState, float]]:\n",
    "\n",
    "        def sample_next_state_reward(state=state) ->\\\n",
    "                Tuple[InventoryState, float]:\n",
    "            demand_sample: int = np.random.poisson(self.poisson_lambda)\n",
    "            ip: int = state.inventory_position()\n",
    "            next_state: InventoryState = InventoryState(\n",
    "                max(ip - demand_sample, 0),\n",
    "                max(self.capacity - ip, 0)\n",
    "            )\n",
    "            reward: float = - self.holding_cost * state.on_hand\\\n",
    "                - self.stockout_cost * max(demand_sample - ip, 0)\n",
    "            return next_state, reward\n",
    "\n",
    "        return SampledDistribution(sample_next_state_reward)\n",
    "\n",
    "\n",
    "class SimpleInventoryMRPFinite(FiniteMarkovRewardProcess[InventoryState]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int,\n",
    "        poisson_lambda: float,\n",
    "        holding_cost: float,\n",
    "        stockout_cost: float\n",
    "    ):\n",
    "        self.capacity: int = capacity\n",
    "        self.poisson_lambda: float = poisson_lambda\n",
    "        self.holding_cost: float = holding_cost\n",
    "        self.stockout_cost: float = stockout_cost\n",
    "\n",
    "        self.poisson_distr = poisson(poisson_lambda)\n",
    "        super().__init__(self.get_transition_reward_map())\n",
    "\n",
    "    def get_transition_reward_map(self) -> RewardTransition[InventoryState]:\n",
    "        d: Dict[InventoryState, Categorical[Tuple[InventoryState, float]]] = {}\n",
    "        for alpha in range(self.capacity + 1):\n",
    "            for beta in range(self.capacity + 1 - alpha):\n",
    "                state = InventoryState(alpha, beta)\n",
    "                ip = state.inventory_position()\n",
    "                beta1 = self.capacity - ip\n",
    "                base_reward = - self.holding_cost * state.on_hand\n",
    "                sr_probs_map: Dict[Tuple[InventoryState, float], float] =\\\n",
    "                    {(InventoryState(ip - i, beta1), base_reward):\n",
    "                     self.poisson_distr.pmf(i) for i in range(ip)}\n",
    "                probability = 1 - self.poisson_distr.cdf(ip - 1)\n",
    "                reward = base_reward - self.stockout_cost *\\\n",
    "                    (probability * (self.poisson_lambda - ip) +\n",
    "                     ip * self.poisson_distr.pmf(ip))\n",
    "                sr_probs_map[(InventoryState(0, beta1), reward)] = probability\n",
    "                d[state] = Categorical(sr_probs_map)\n",
    "        return d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    user_capacity = 2\n",
    "    user_poisson_lambda = 1.0\n",
    "    user_holding_cost = 1.0\n",
    "    user_stockout_cost = 10.0\n",
    "\n",
    "    user_gamma = 0.9\n",
    "\n",
    "    si_mrp = SimpleInventoryMRPFinite(\n",
    "        capacity=user_capacity,\n",
    "        poisson_lambda=user_poisson_lambda,\n",
    "        holding_cost=user_holding_cost,\n",
    "        stockout_cost=user_stockout_cost\n",
    "    )\n",
    "\n",
    "    from rl.markov_process import FiniteMarkovProcess\n",
    "    print(\"Transition Map\")\n",
    "    print(\"--------------\")\n",
    "    print(FiniteMarkovProcess(si_mrp.transition_map))\n",
    "\n",
    "    print(\"Transition Reward Map\")\n",
    "    print(\"---------------------\")\n",
    "    print(si_mrp)\n",
    "\n",
    "    print(\"Stationary Distribution\")\n",
    "    print(\"-----------------------\")\n",
    "    si_mrp.display_stationary_distribution()\n",
    "    print()\n",
    "\n",
    "    print(\"Reward Function\")\n",
    "    print(\"---------------\")\n",
    "    si_mrp.display_reward_function()\n",
    "    print()\n",
    "\n",
    "    print(\"Value Function\")\n",
    "    print(\"--------------\")\n",
    "    si_mrp.display_value_function(gamma=user_gamma)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
