{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Generator,Callable,Optional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "from rl.markov_process import MarkovRewardProcess,FiniteMarkovProcess,MarkovProcess\n",
    "from rl.markov_process import FiniteMarkovRewardProcess\n",
    "from rl.markov_process import Transition,TransitionStep,ReturnStep\n",
    "from rl.markov_process import RewardTransition\n",
    "from rl.distribution import Constant,Categorical,SampledDistribution,Distribution\n",
    "from rl.gen_utils.common_funcs import get_logistic_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the procedure in textbook, we can have:\n",
    "<br>\n",
    "$E[log(X_T)=log(V_g)]+(r+{ \\mu-r \\over 2 \\sigma^2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rl.distribution'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-13a4fc8ad640>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdataclasses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataclass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSampledDistribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mChoose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGaussian\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkov_decision_process\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMarkovDecisionProcess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_approx\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDNNSpec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdamGradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDNNApprox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rl.distribution'"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Sequence, Callable, Tuple, Iterator\n",
    "from rl.distribution import Distribution, SampledDistribution, Choose, Gaussian\n",
    "from rl.markov_decision_process import MarkovDecisionProcess, Policy\n",
    "from rl.function_approx import DNNSpec, AdamGradient, DNNApprox\n",
    "from rl.approximate_dynamic_programming import back_opt_vf_and_policy\n",
    "from rl.approximate_dynamic_programming import back_opt_qvf\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class AssetAllocDiscrete:\n",
    "    risky_return_distributions: Sequence[Distribution[float]]\n",
    "    riskless_returns: Sequence[float]\n",
    "    utility_func: Callable[[float], float]\n",
    "    risky_alloc_choices: Sequence[float]\n",
    "    feature_functions: Sequence[Callable[[Tuple[float, float]], float]]\n",
    "    dnn_spec: DNNSpec\n",
    "    initial_wealth_distribution: Distribution[float]\n",
    "\n",
    "    def time_steps(self) -> int:\n",
    "        return len(self.risky_return_distributions)\n",
    "\n",
    "    def uniform_actions(self) -> Choose[float]:\n",
    "        return Choose(set(self.risky_alloc_choices))\n",
    "\n",
    "    def get_mdp(self, t: int) -> MarkovDecisionProcess[float, float]:\n",
    "        \"\"\"\n",
    "        State is Wealth W_t, Action is investment in risky asset (= x_t)\n",
    "        Investment in riskless asset is W_t - x_t\n",
    "        \"\"\"\n",
    "\n",
    "        distr: Distribution[float] = self.risky_return_distributions[t]\n",
    "        rate: float = self.riskless_returns[t]\n",
    "        alloc_choices: Sequence[float] = self.risky_alloc_choices\n",
    "        steps: int = self.time_steps()\n",
    "        utility_f: Callable[[float], float] = self.utility_func\n",
    "\n",
    "        class AssetAllocMDP(MarkovDecisionProcess[float, float]):\n",
    "\n",
    "            def step(\n",
    "                self,\n",
    "                wealth: float,\n",
    "                alloc: float\n",
    "            ) -> SampledDistribution[Tuple[float, float]]:\n",
    "\n",
    "                def sr_sampler_func(\n",
    "                    wealth=wealth,\n",
    "                    alloc=alloc\n",
    "                ) -> Tuple[float, float]:\n",
    "                    next_wealth: float = alloc * (1 + distr.sample()) \\\n",
    "                        + (wealth - alloc) * (1 + rate)\n",
    "                    reward: float = utility_f(next_wealth) \\\n",
    "                        if t == steps - 1 else 0.\n",
    "                    return (next_wealth, reward)\n",
    "\n",
    "                return SampledDistribution(\n",
    "                    sampler=sr_sampler_func,\n",
    "                    expectation_samples=1000\n",
    "                )\n",
    "\n",
    "            def actions(self, wealth: float) -> Sequence[float]:\n",
    "                return alloc_choices\n",
    "\n",
    "        return AssetAllocMDP()\n",
    "\n",
    "    def get_qvf_func_approx(self) -> DNNApprox[Tuple[float, float]]:\n",
    "\n",
    "        adam_gradient: AdamGradient = AdamGradient(\n",
    "            learning_rate=0.1,\n",
    "            decay1=0.9,\n",
    "            decay2=0.999\n",
    "        )\n",
    "        return DNNApprox.create(\n",
    "            feature_functions=self.feature_functions,\n",
    "            dnn_spec=self.dnn_spec,\n",
    "            adam_gradient=adam_gradient\n",
    "        )\n",
    "\n",
    "    def get_states_distribution(self, t: int) -> SampledDistribution[float]:\n",
    "\n",
    "        actions_distr: Choose[float] = self.uniform_actions()\n",
    "\n",
    "        def states_sampler_func() -> float:\n",
    "            wealth: float = self.initial_wealth_distribution.sample()\n",
    "            for i in range(t):\n",
    "                distr: Distribution[float] = self.risky_return_distributions[i]\n",
    "                rate: float = self.riskless_returns[i]\n",
    "                alloc: float = actions_distr.sample()\n",
    "                wealth = alloc * (1 + distr.sample()) + \\\n",
    "                    (wealth - alloc) * (1 + rate)\n",
    "            return wealth\n",
    "\n",
    "        return SampledDistribution(states_sampler_func)\n",
    "\n",
    "    def backward_induction_qvf(self) -> \\\n",
    "            Iterator[DNNApprox[Tuple[float, float]]]:\n",
    "\n",
    "        init_fa: DNNApprox[Tuple[float, float]] = self.get_qvf_func_approx()\n",
    "\n",
    "        mdp_f0_mu_triples: Sequence[Tuple[\n",
    "            MarkovDecisionProcess[float, float],\n",
    "            DNNApprox[Tuple[float, float]],\n",
    "            SampledDistribution[float]\n",
    "        ]] = [(\n",
    "            self.get_mdp(i),\n",
    "            init_fa,\n",
    "            self.get_states_distribution(i)\n",
    "        ) for i in range(self.time_steps())]\n",
    "\n",
    "        num_state_samples: int = 300\n",
    "        error_tolerance: float = 1e-6\n",
    "\n",
    "        return back_opt_qvf(\n",
    "            mdp_f0_mu_triples=mdp_f0_mu_triples,\n",
    "            γ=1.0,\n",
    "            num_state_samples=num_state_samples,\n",
    "            error_tolerance=error_tolerance\n",
    "        )\n",
    "\n",
    "    def get_vf_func_approx(\n",
    "        self,\n",
    "        ff: Sequence[Callable[[float], float]]\n",
    "    ) -> DNNApprox[float]:\n",
    "\n",
    "        adam_gradient: AdamGradient = AdamGradient(\n",
    "            learning_rate=0.1,\n",
    "            decay1=0.9,\n",
    "            decay2=0.999\n",
    "        )\n",
    "        return DNNApprox.create(\n",
    "            feature_functions=ff,\n",
    "            dnn_spec=self.dnn_spec,\n",
    "            adam_gradient=adam_gradient\n",
    "        )\n",
    "\n",
    "    def backward_induction_vf_and_pi(\n",
    "        self,\n",
    "        ff: Sequence[Callable[[float], float]]\n",
    "    ) -> Iterator[Tuple[DNNApprox[float], Policy[float, float]]]:\n",
    "\n",
    "        init_fa: DNNApprox[float] = self.get_vf_func_approx(ff)\n",
    "\n",
    "        mdp_f0_mu_triples: Sequence[Tuple[\n",
    "            MarkovDecisionProcess[float, float],\n",
    "            DNNApprox[float],\n",
    "            SampledDistribution[float]\n",
    "        ]] = [(\n",
    "            self.get_mdp(i),\n",
    "            init_fa,\n",
    "            self.get_states_distribution(i)\n",
    "        ) for i in range(self.time_steps())]\n",
    "\n",
    "        num_state_samples: int = 300\n",
    "        error_tolerance: float = 1e-8\n",
    "\n",
    "        return back_opt_vf_and_policy(\n",
    "            mdp_f0_mu_triples=mdp_f0_mu_triples,\n",
    "            γ=1.0,\n",
    "            num_state_samples=num_state_samples,\n",
    "            error_tolerance=error_tolerance\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    from pprint import pprint\n",
    "\n",
    "    steps: int = 4\n",
    "    μ: float = 0.13\n",
    "    σ: float = 0.2\n",
    "    r: float = 0.07\n",
    "    a: float = 1.0\n",
    "    init_wealth: float = 1.0\n",
    "    init_wealth_var: float = 0.1\n",
    "\n",
    "    excess: float = μ - r\n",
    "    var: float = σ * σ\n",
    "    base_alloc: float = excess / (a * var)\n",
    "\n",
    "    risky_ret: Sequence[Gaussian] = [Gaussian(μ=μ, σ=σ) for _ in range(steps)]\n",
    "    riskless_ret: Sequence[float] = [r for _ in range(steps)]\n",
    "    utility_function: Callable[[float], float] = lambda x: - np.exp(-a * x) / a\n",
    "    alloc_choices: Sequence[float] = np.linspace(\n",
    "        2 / 3 * base_alloc,\n",
    "        4 / 3 * base_alloc,\n",
    "        11\n",
    "    )\n",
    "    feature_funcs: Sequence[Callable[[Tuple[float, float]], float]] = \\\n",
    "        [\n",
    "            lambda _: 1.,\n",
    "            lambda w_x: w_x[0],\n",
    "            lambda w_x: w_x[1],\n",
    "            lambda w_x: w_x[1] * w_x[1]\n",
    "        ]\n",
    "    dnn: DNNSpec = DNNSpec(\n",
    "        neurons=[],\n",
    "        bias=False,\n",
    "        hidden_activation=lambda x: x,\n",
    "        hidden_activation_deriv=lambda y: np.ones_like(y),\n",
    "        output_activation=lambda x: - np.sign(a) * np.exp(-x),\n",
    "        output_activation_deriv=lambda y: -y\n",
    "    )\n",
    "    init_wealth_distr: Gaussian = Gaussian(μ=init_wealth, σ=init_wealth_var)\n",
    "\n",
    "    aad: AssetAllocDiscrete = AssetAllocDiscrete(\n",
    "        risky_return_distributions=risky_ret,\n",
    "        riskless_returns=riskless_ret,\n",
    "        utility_func=utility_function,\n",
    "        risky_alloc_choices=alloc_choices,\n",
    "        feature_functions=feature_funcs,\n",
    "        dnn_spec=dnn,\n",
    "        initial_wealth_distribution=init_wealth_distr\n",
    "    )\n",
    "\n",
    "    # vf_ff: Sequence[Callable[[float], float]] = [lambda _: 1., lambda w: w]\n",
    "    # it_vf: Iterator[Tuple[DNNApprox[float], Policy[float, float]]] = \\\n",
    "    #     aad.backward_induction_vf_and_pi(vf_ff)\n",
    "\n",
    "    # print(\"Backward Induction: VF And Policy\")\n",
    "    # print(\"---------------------------------\")\n",
    "    # print()\n",
    "    # for t, (v, p) in enumerate(it_vf):\n",
    "    #     print(f\"Time {t:d}\")\n",
    "    #     print()\n",
    "    #     opt_alloc: float = p.act(init_wealth).value\n",
    "    #     val: float = v.evaluate([init_wealth])[0]\n",
    "    #     print(f\"Opt Risky Allocation = {opt_alloc:.2f}, Opt Val = {val:.3f}\")\n",
    "    #     print(\"Weights\")\n",
    "    #     for w in v.weights:\n",
    "    #         print(w.weights)\n",
    "    #     print()\n",
    "\n",
    "    it_qvf: Iterator[DNNApprox[Tuple[float, float]]] = \\\n",
    "        aad.backward_induction_qvf()\n",
    "\n",
    "    print(\"Backward Induction on Q-Value Function\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print()\n",
    "    for t, q in enumerate(it_qvf):\n",
    "        print(f\"Time {t:d}\")\n",
    "        print()\n",
    "        opt_alloc: float = max(\n",
    "            ((q.evaluate([(init_wealth, ac)])[0], ac) for ac in alloc_choices),\n",
    "            key=itemgetter(0)\n",
    "        )[1]\n",
    "        val: float = max(q.evaluate([(init_wealth, ac)])[0]\n",
    "                         for ac in alloc_choices)\n",
    "        print(f\"Opt Risky Allocation = {opt_alloc:.3f}, Opt Val = {val:.3f}\")\n",
    "        print(\"Optimal Weights below:\")\n",
    "        for wts in q.weights:\n",
    "            pprint(wts.weights)\n",
    "        print()\n",
    "\n",
    "    print(\"Analytical Solution\")\n",
    "    print(\"-------------------\")\n",
    "    print()\n",
    "\n",
    "    for t in range(steps):\n",
    "        print(f\"Time {t:d}\")\n",
    "        print()\n",
    "        left: int = steps - t\n",
    "        growth: float = (1 + r) ** (left - 1)\n",
    "        alloc: float = base_alloc / growth\n",
    "        val: float = - np.exp(- excess * excess * left / (2 * var)\n",
    "                              - a * growth * (1 + r) * init_wealth) / a\n",
    "        bias_wt: float = excess * excess * (left - 1) / (2 * var) + \\\n",
    "            np.log(np.abs(a))\n",
    "        w_t_wt: float = a * growth * (1 + r)\n",
    "        x_t_wt: float = a * excess * growth\n",
    "        x_t2_wt: float = - var * (a * growth) ** 2 / 2\n",
    "\n",
    "        print(f\"Opt Risky Allocation = {alloc:.3f}, Opt Val = {val:.3f}\")\n",
    "        print(f\"Bias Weight = {bias_wt:.3f}\")\n",
    "        print(f\"W_t Weight = {w_t_wt:.3f}\")\n",
    "        print(f\"x_t Weight = {x_t_wt:.3f}\")\n",
    "        print(f\"x_t^2 Weight = {x_t2_wt:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
