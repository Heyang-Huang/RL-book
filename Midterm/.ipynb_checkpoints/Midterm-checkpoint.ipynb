{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1      $\\quad$ $\\quad$ $\\quad$  $\\quad$      Heyang   Huang    (heyangh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Mapping,Dict\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess,StateActionMapping\n",
    "from rl. dynamic_programming import value_iteration_result,value_iteration\n",
    "from rl.distribution import Categorical\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State Space: \n",
    "$S=\\{(x,y)|0<=x,y<=7\\ and (x,y)\\in \\{open \\, space\\}\\}$ with each state(x,y) representing the coordinate of a location in maze <br>Or more specifically:$S=\\{(0, 0),\n",
    "(0, 2),\n",
    "(0, 3),\n",
    "(0, 4),\n",
    "(0, 5),\n",
    "(0, 6),\n",
    "(0, 7),\n",
    "(1, 0),\n",
    "(1, 3),\n",
    "(2, 0),\n",
    "(2, 2),\n",
    "(2, 3),\n",
    "(2, 4),\n",
    "(2, 5),\n",
    "(2, 7),\n",
    "(3, 0),\n",
    "(3, 1),\n",
    "(3, 2),\n",
    "(3, 5),\n",
    "(3, 7),\n",
    "(4, 0),\n",
    "(4, 2),\n",
    "(4, 4),\n",
    "(4, 5),\n",
    "(4, 6),\n",
    "(4, 7),\n",
    "(5, 2),\n",
    "(5, 4),\n",
    "(5, 6),\n",
    "(6, 0),\n",
    "(6, 4),\n",
    "(6, 6),\n",
    "(6, 7),\n",
    "(7, 0),\n",
    "(7, 1),\n",
    "(7, 2),\n",
    "(7, 3),\n",
    "(7, 4),\n",
    "(7, 7)\\}$ Where (7,7) is a terminating state\n",
    "####  Action Space: \n",
    "Action space at each state will be a subset of {(1,0),(-1,0),(0,1),(0,-1)},which represents {right,left,up,down} respectively\n",
    "<br>$A((x,y))=\\{D| D\\subseteq \\{(1,0),(-1,0ï¼‰,(0,1),(0,-1)\\} \\text{ and } \\forall d \\in D \\text{ , } d+(x,y) \\in S\\}$\n",
    "In other words, action space at each location (x,y) is: {one step up, down,left,or right as long as stay in state space S} \n",
    "####  State Transition Probability Function: \n",
    "$Pr[S'|S,A]=     \n",
    "    \\begin{cases}\n",
    "      {1 } & \\text{if} \\quad S+A=S'\\\\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases} \\quad \\text{for all Non-terminating (x,y) in S. (for instance (7,0)+(0,1)=(7,1))}$\n",
    "#### Reward Transition Function 1:\n",
    "<br><br> $R(S,A,S')=-1 \\quad \\forall (S,A,S')$  <br>$\\gamma=1$  \n",
    "All transitions have reward -1\n",
    "#### Reward Transition Function 2:  \n",
    "$R(S,A,S')=  \\begin{cases}\n",
    "      {1 } & \\text{if  } S' \\in T\\\\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}\t\\quad \\gamma=0.9$  \n",
    "All transitions that don't lead to the terminating state have a reward 0, while transition to the terminating state has a reward 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE = 'SPACE'\n",
    "BLOCK = 'BLOCK'\n",
    "GOAL = 'GOAL'\n",
    "\n",
    "maze_grid = {(0, 0): SPACE, (0, 1): BLOCK, (0, 2): SPACE, (0, 3): SPACE, (0, 4): SPACE, \n",
    "             (0, 5): SPACE, (0, 6): SPACE, (0, 7): SPACE, (1, 0): SPACE, (1, 1): BLOCK,\n",
    "             (1, 2): BLOCK, (1, 3): SPACE, (1, 4): BLOCK, (1, 5): BLOCK, (1, 6): BLOCK, \n",
    "             (1, 7): BLOCK, (2, 0): SPACE, (2, 1): BLOCK, (2, 2): SPACE, (2, 3): SPACE, \n",
    "             (2, 4): SPACE, (2, 5): SPACE, (2, 6): BLOCK, (2, 7): SPACE, (3, 0): SPACE, \n",
    "             (3, 1): SPACE, (3, 2): SPACE, (3, 3): BLOCK, (3, 4): BLOCK, (3, 5): SPACE, \n",
    "             (3, 6): BLOCK, (3, 7): SPACE, (4, 0): SPACE, (4, 1): BLOCK, (4, 2): SPACE, \n",
    "             (4, 3): BLOCK, (4, 4): SPACE, (4, 5): SPACE, (4, 6): SPACE, (4, 7): SPACE, \n",
    "             (5, 0): BLOCK, (5, 1): BLOCK, (5, 2): SPACE, (5, 3): BLOCK, (5, 4): SPACE, \n",
    "             (5, 5): BLOCK, (5, 6): SPACE, (5, 7): BLOCK, (6, 0): SPACE, (6, 1): BLOCK, \n",
    "             (6, 2): BLOCK, (6, 3): BLOCK, (6, 4): SPACE, (6, 5): BLOCK, (6, 6): SPACE, \n",
    "             (6, 7): SPACE, (7, 0): SPACE, (7, 1): SPACE, (7, 2): SPACE, (7, 3): SPACE, \n",
    "             (7, 4): SPACE, (7, 5): BLOCK, (7, 6): BLOCK, (7, 7): GOAL}\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class MazeState:\n",
    "    '''A non-block location in Maze\n",
    "    '''\n",
    "    coordinate: Tuple[int,int]\n",
    "\n",
    "MazeMapping = StateActionMapping[MazeState, MazeState]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeMDPFinite(FiniteMarkovDecisionProcess[MazeState, Tuple[int,int]]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        maze_grid: Dict,\n",
    "        reward_formulation: int  # a flag to specify which formulation is being implemented.\n",
    "    ):\n",
    "        self.maze_grid=maze_grid\n",
    "        self.reward_formulation=reward_formulation\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "    \n",
    "    \n",
    "    def get_action_transition_reward_map(self) -> MazeMapping:\n",
    "        # Notice that Action space is actually a set of MazeStates\n",
    "        d: Dict[MazeState, Dict[MazeState, Categorical[Tuple[MazeState,float]]]] = {}\n",
    "        non_t_states=[] # non terminating states\n",
    "        all_states_coor=[]# all states\n",
    "        # Retrieve valid states from all maze_grid lcoations \n",
    "        for i in self.maze_grid:\n",
    "            if self.maze_grid[i]=='SPACE':\n",
    "                non_t_states.append(MazeState(i))\n",
    "                all_states_coor.append(i)\n",
    "            elif self.maze_grid[i]=='GOAL':\n",
    "                #store the terminating state and append it to S\n",
    "                t_state_coor=i \n",
    "                all_states_coor.append(i)\n",
    "                \n",
    "        # use first formulation\n",
    "        if self.reward_formulation==1:\n",
    "            reward=-1\n",
    "            for current_s in non_t_states:\n",
    "                d1: Dict[MazeState, Categorical[Tuple[MazeState, float]]] = {}\n",
    "                #x,y+1\n",
    "                nxt_coor=(current_s.coordinate[0],current_s.coordinate[1]+1)\n",
    "                if nxt_coor in all_states_coor:\n",
    "                    nxt_s=MazeState(nxt_coor)\n",
    "                    d1[(0,1)]=Categorical({(nxt_s,reward):1})\n",
    "                #x,y-1\n",
    "                nxt_coor=(current_s.coordinate[0],current_s.coordinate[1]-1)\n",
    "                if nxt_coor in all_states_coor:\n",
    "                    nxt_s=MazeState(nxt_coor)\n",
    "                    d1[(0,-1)]=Categorical({(nxt_s,reward):1})\n",
    "                #x+1,y\n",
    "                nxt_coor=(current_s.coordinate[0]+1,current_s.coordinate[1])\n",
    "                if nxt_coor in all_states_coor:\n",
    "                    nxt_s=MazeState(nxt_coor)\n",
    "                    d1[(1,0)]=Categorical({(nxt_s,reward):1})\n",
    "                #x-1,y\n",
    "                nxt_coor=(current_s.coordinate[0]-1,current_s.coordinate[1])\n",
    "                if nxt_coor in all_states_coor:\n",
    "                    nxt_s=MazeState(nxt_coor)\n",
    "                    d1[(-1,0)]=Categorical({(nxt_s,reward):1})\n",
    "                d[current_s]=d1\n",
    "                \n",
    "        # use second formulation\n",
    "        if self.reward_formulation==2:\n",
    "            reward_non_t=0\n",
    "            reward_t=1\n",
    "            for current_s in non_t_states:\n",
    "                d1: Dict[MazeState, Categorical[Tuple[MazeState, float]]] = {}\n",
    "                #x,y+1\n",
    "                nxt_coor=(current_s.coordinate[0],current_s.coordinate[1]+1)\n",
    "                # if nxt_step is a non-block state, give rewards based on whether it's the terminating state\n",
    "                if nxt_coor in all_states_coor:\n",
    "                    nxt_s=MazeState(nxt_coor)\n",
    "                    if  t_state_coor==nxt_coor:\n",
    "                        d1[(0,1)]=Categorical({(nxt_s,reward_t):1})\n",
    "                    else:\n",
    "                        d1[(0,1)]=Categorical({(nxt_s,reward_non_t):1})\n",
    "                #x,y-1\n",
    "                nxt_coor=(current_s.coordinate[0],current_s.coordinate[1]-1)\n",
    "                 # if nxt_step is a non-block state, give rewards based on whether it's the terminating state\n",
    "                if nxt_coor in all_states_coor:\n",
    "                    nxt_s=MazeState(nxt_coor)\n",
    "                    if  t_state_coor==nxt_coor:\n",
    "                        d1[(0,-1)]=Categorical({(nxt_s,reward_t):1})\n",
    "                    else:\n",
    "                        d1[(0,-1)]=Categorical({(nxt_s,reward_non_t):1})\n",
    "                #x+1,y\n",
    "                nxt_coor=(current_s.coordinate[0]+1,current_s.coordinate[1])\n",
    "                 # if nxt_step is a non-block state, give rewards based on whether it's the terminating state\n",
    "                if nxt_coor in all_states_coor:\n",
    "                    nxt_s=MazeState(nxt_coor)\n",
    "                    if  t_state_coor==nxt_coor:\n",
    "                        d1[(1,0)]=Categorical({(nxt_s,reward_t):1})\n",
    "                    else:\n",
    "                        d1[(1,0)]=Categorical({(nxt_s,reward_non_t):1})\n",
    "                #x-1,y\n",
    "                nxt_coor=(current_s.coordinate[0]-1,current_s.coordinate[1])\n",
    "                 # if nxt_step is a non-block state, give rewards based on whether it's the terminating state\n",
    "                if nxt_coor in all_states_coor:\n",
    "                    nxt_s=MazeState(nxt_coor)\n",
    "                    if  t_state_coor==nxt_coor:\n",
    "                        d1[(-1,0)]=Categorical({(nxt_s,reward_t):1})\n",
    "                    else:\n",
    "                        d1[(-1,0)]=Categorical({(nxt_s,reward_non_t):1})\n",
    "                d[current_s]=d1\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Iterations: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({MazeState(coordinate=(0, 0)): -16.0,\n",
       "  MazeState(coordinate=(0, 2)): -12.0,\n",
       "  MazeState(coordinate=(0, 3)): -11.0,\n",
       "  MazeState(coordinate=(0, 4)): -12.0,\n",
       "  MazeState(coordinate=(0, 5)): -13.0,\n",
       "  MazeState(coordinate=(0, 6)): -14.0,\n",
       "  MazeState(coordinate=(0, 7)): -15.0,\n",
       "  MazeState(coordinate=(1, 0)): -15.0,\n",
       "  MazeState(coordinate=(1, 3)): -10.0,\n",
       "  MazeState(coordinate=(2, 0)): -14.0,\n",
       "  MazeState(coordinate=(2, 2)): -10.0,\n",
       "  MazeState(coordinate=(2, 3)): -9.0,\n",
       "  MazeState(coordinate=(2, 4)): -8.0,\n",
       "  MazeState(coordinate=(2, 5)): -7.0,\n",
       "  MazeState(coordinate=(2, 7)): -7.0,\n",
       "  MazeState(coordinate=(3, 0)): -13.0,\n",
       "  MazeState(coordinate=(3, 1)): -12.0,\n",
       "  MazeState(coordinate=(3, 2)): -11.0,\n",
       "  MazeState(coordinate=(3, 5)): -6.0,\n",
       "  MazeState(coordinate=(3, 7)): -6.0,\n",
       "  MazeState(coordinate=(4, 0)): -14.0,\n",
       "  MazeState(coordinate=(4, 2)): -12.0,\n",
       "  MazeState(coordinate=(4, 4)): -6.0,\n",
       "  MazeState(coordinate=(4, 5)): -5.0,\n",
       "  MazeState(coordinate=(4, 6)): -4.0,\n",
       "  MazeState(coordinate=(4, 7)): -5.0,\n",
       "  MazeState(coordinate=(5, 2)): -13.0,\n",
       "  MazeState(coordinate=(5, 4)): -7.0,\n",
       "  MazeState(coordinate=(5, 6)): -3.0,\n",
       "  MazeState(coordinate=(6, 0)): -14.0,\n",
       "  MazeState(coordinate=(6, 4)): -8.0,\n",
       "  MazeState(coordinate=(6, 6)): -2.0,\n",
       "  MazeState(coordinate=(6, 7)): -1.0,\n",
       "  MazeState(coordinate=(7, 0)): -13.0,\n",
       "  MazeState(coordinate=(7, 1)): -12.0,\n",
       "  MazeState(coordinate=(7, 2)): -11.0,\n",
       "  MazeState(coordinate=(7, 3)): -10.0,\n",
       "  MazeState(coordinate=(7, 4)): -9.0},\n",
       " For State MazeState(coordinate=(0, 0)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(0, 2)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(0, 3)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(0, 4)):\n",
       "   Do Action (0, -1) with Probability 1.000\n",
       " For State MazeState(coordinate=(0, 5)):\n",
       "   Do Action (0, -1) with Probability 1.000\n",
       " For State MazeState(coordinate=(0, 6)):\n",
       "   Do Action (0, -1) with Probability 1.000\n",
       " For State MazeState(coordinate=(0, 7)):\n",
       "   Do Action (0, -1) with Probability 1.000\n",
       " For State MazeState(coordinate=(1, 0)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(1, 3)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(2, 0)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(2, 2)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(2, 3)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(2, 4)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(2, 5)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(2, 7)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(3, 0)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(3, 1)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(3, 2)):\n",
       "   Do Action (-1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(3, 5)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(3, 7)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(4, 0)):\n",
       "   Do Action (-1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(4, 2)):\n",
       "   Do Action (-1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(4, 4)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(4, 5)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(4, 6)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(4, 7)):\n",
       "   Do Action (0, -1) with Probability 1.000\n",
       " For State MazeState(coordinate=(5, 2)):\n",
       "   Do Action (-1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(5, 4)):\n",
       "   Do Action (-1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(5, 6)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(6, 0)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(6, 4)):\n",
       "   Do Action (-1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(6, 6)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(6, 7)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(7, 0)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(7, 1)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(7, 2)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(7, 3)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(7, 4)):\n",
       "   Do Action (-1, 0) with Probability 1.000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formulation 1\n",
    "new_maze=MazeMDPFinite(maze_grid=maze_grid,reward_formulation=1)\n",
    "value_iteration_result(new_maze,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Iterations: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({MazeState(coordinate=(0, 0)): 0.2058911320946491,\n",
       "  MazeState(coordinate=(0, 2)): 0.31381059609000017,\n",
       "  MazeState(coordinate=(0, 3)): 0.34867844010000015,\n",
       "  MazeState(coordinate=(0, 4)): 0.31381059609000017,\n",
       "  MazeState(coordinate=(0, 5)): 0.28242953648100017,\n",
       "  MazeState(coordinate=(0, 6)): 0.25418658283290013,\n",
       "  MazeState(coordinate=(0, 7)): 0.22876792454961012,\n",
       "  MazeState(coordinate=(1, 0)): 0.22876792454961012,\n",
       "  MazeState(coordinate=(1, 3)): 0.38742048900000015,\n",
       "  MazeState(coordinate=(2, 0)): 0.25418658283290013,\n",
       "  MazeState(coordinate=(2, 2)): 0.38742048900000015,\n",
       "  MazeState(coordinate=(2, 3)): 0.43046721000000016,\n",
       "  MazeState(coordinate=(2, 4)): 0.47829690000000014,\n",
       "  MazeState(coordinate=(2, 5)): 0.5314410000000002,\n",
       "  MazeState(coordinate=(2, 7)): 0.5314410000000002,\n",
       "  MazeState(coordinate=(3, 0)): 0.28242953648100017,\n",
       "  MazeState(coordinate=(3, 1)): 0.31381059609000017,\n",
       "  MazeState(coordinate=(3, 2)): 0.34867844010000015,\n",
       "  MazeState(coordinate=(3, 5)): 0.5904900000000002,\n",
       "  MazeState(coordinate=(3, 7)): 0.5904900000000002,\n",
       "  MazeState(coordinate=(4, 0)): 0.25418658283290013,\n",
       "  MazeState(coordinate=(4, 2)): 0.31381059609000017,\n",
       "  MazeState(coordinate=(4, 4)): 0.5904900000000002,\n",
       "  MazeState(coordinate=(4, 5)): 0.6561000000000001,\n",
       "  MazeState(coordinate=(4, 6)): 0.7290000000000001,\n",
       "  MazeState(coordinate=(4, 7)): 0.6561000000000001,\n",
       "  MazeState(coordinate=(5, 2)): 0.28242953648100017,\n",
       "  MazeState(coordinate=(5, 4)): 0.5314410000000002,\n",
       "  MazeState(coordinate=(5, 6)): 0.81,\n",
       "  MazeState(coordinate=(6, 0)): 0.25418658283290013,\n",
       "  MazeState(coordinate=(6, 4)): 0.47829690000000014,\n",
       "  MazeState(coordinate=(6, 6)): 0.9,\n",
       "  MazeState(coordinate=(6, 7)): 1.0,\n",
       "  MazeState(coordinate=(7, 0)): 0.28242953648100017,\n",
       "  MazeState(coordinate=(7, 1)): 0.31381059609000017,\n",
       "  MazeState(coordinate=(7, 2)): 0.34867844010000015,\n",
       "  MazeState(coordinate=(7, 3)): 0.38742048900000015,\n",
       "  MazeState(coordinate=(7, 4)): 0.43046721000000016},\n",
       " For State MazeState(coordinate=(0, 0)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(0, 2)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(0, 3)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(0, 4)):\n",
       "   Do Action (0, -1) with Probability 1.000\n",
       " For State MazeState(coordinate=(0, 5)):\n",
       "   Do Action (0, -1) with Probability 1.000\n",
       " For State MazeState(coordinate=(0, 6)):\n",
       "   Do Action (0, -1) with Probability 1.000\n",
       " For State MazeState(coordinate=(0, 7)):\n",
       "   Do Action (0, -1) with Probability 1.000\n",
       " For State MazeState(coordinate=(1, 0)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(1, 3)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(2, 0)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(2, 2)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(2, 3)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(2, 4)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(2, 5)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(2, 7)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(3, 0)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(3, 1)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(3, 2)):\n",
       "   Do Action (-1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(3, 5)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(3, 7)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(4, 0)):\n",
       "   Do Action (-1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(4, 2)):\n",
       "   Do Action (-1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(4, 4)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(4, 5)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(4, 6)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(4, 7)):\n",
       "   Do Action (0, -1) with Probability 1.000\n",
       " For State MazeState(coordinate=(5, 2)):\n",
       "   Do Action (-1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(5, 4)):\n",
       "   Do Action (-1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(5, 6)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(6, 0)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(6, 4)):\n",
       "   Do Action (-1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(6, 6)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(6, 7)):\n",
       "   Do Action (1, 0) with Probability 1.000\n",
       " For State MazeState(coordinate=(7, 0)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(7, 1)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(7, 2)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(7, 3)):\n",
       "   Do Action (0, 1) with Probability 1.000\n",
       " For State MazeState(coordinate=(7, 4)):\n",
       "   Do Action (-1, 0) with Probability 1.000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# formulation 2\n",
    "new_maze=MazeMDPFinite(maze_grid=maze_grid,reward_formulation=2)\n",
    "value_iteration_result(new_maze,0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As shown above, the $\\pi ^*$ for both formulations are the same\n",
    "#### The two formulations take same number of iterations to converge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V=R+\\gamma PV \\Rightarrow V=(I-\\gamma P)^{-1}R$\n",
    "<br>Exactly representing $V$ with $\\phi$ in a linear relationship is equivalent to:\n",
    "<br>$(I-\\gamma P)^{-1}_{n*n}R_{n*1}=\\phi_{n*m} w_{m*1}$\n",
    "<br><br> Minimal Requirment: <br>(1): $(I-\\gamma P)^{-1}_{n*n}R_{n*1}$ is in the column space of $\\phi_{n*m}$\n",
    "<br>(2): Square matrix $(I-\\gamma P)_{n*n}$ should be full rank \n",
    "<br> (3): All $m$ basis vectors:$\\phi_i$ in $\\phi_{n*m}$ should be linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State Space: \n",
    "Hourly wage each day: $S=\\{1,2,3,4......W\\}$  W is a absorbing state. But I will let it redirect to itself with p=1, rather than terminate the MDP directy for two reasons: <br>1) I need to learn to spend all my time H working on current job when S reaches and stays at W. <br> 2) We are calculating the cumulative reward in a infinite time span. \n",
    "\n",
    "####  Action Space: \n",
    "$A=\\{(l,s)|l \\in Z_{\\geq 0}\\text{ and } s \\in Z_{\\geq 0} \\text{ and }l+s\\leq H\\}$\n",
    "#### Reward and Discount Factor\n",
    "$R(S(w),A(l,s),S'(w'))=w*(H-s-l)$ and   $Discount Factor=\\gamma$\n",
    "#### State-transition Probability\n",
    "$Pr[S'(w')|S(w),A(l,s)]=Pr[w'=min(W,max(w+poisson(a*l),I(\\text{New Job Offer})(w+1)))]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class WageState:\n",
    "    '''Current Wage in the morning after see email\n",
    "    '''\n",
    "    wage: int  \n",
    "\n",
    "# Actions are in form of (l,s)\n",
    "WageMapping = StateActionMapping[WageState,Tuple[int,int]]\n",
    "\n",
    "class WageMDPFinite(FiniteMarkovDecisionProcess[WageState,Tuple[int,int]]):\n",
    "    '''\n",
    "    This class models the career optimizatio problem in Finite MDP. Since I implement the \n",
    "    FiniteMarkovDecisionProcess interface, I can easily solve for optimal valution \n",
    "    function and optimal policy with value_iteration_result() function iin dynamics_programming.py\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        H: int,\n",
    "        W: int,\n",
    "        a: float,\n",
    "        beta: float\n",
    "    ):\n",
    "        self.H: int = H\n",
    "        self.W: float = W\n",
    "        self.a: float = a\n",
    "        self.beta: float = beta\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> WageMapping:\n",
    "        d: Dict[WageState, Dict[Tuple[int,int], Categorical[Tuple[WageState,float]]]] = {}\n",
    "        '''\n",
    "            Generate action_transition_reward_map\n",
    "        '''\n",
    "        for w in range(1,self.W+1):\n",
    "            current_state: WageState = WageState(w)\n",
    "            #d1 is used to store each action's state_transition distribution.\n",
    "            d1: Dict[Tuple[int,int], Categorical[Tuple[WageState, float]]] = {}\n",
    "            # For each action(l,s), feed the distribution of (S' and reward) into d1\n",
    "            for l in range(0,self.H+1):\n",
    "                for s in range(0,self.H+1-l):\n",
    "                    # probability of receiving a new job email tomorrow. \n",
    "                    p_new_job: float=self.beta*s/self.H\n",
    "                    # today's reward\n",
    "                    reward: float = w*(self.H-s-l)\n",
    "                    # store the distribution of (S',reward) into sr_probs_dict \n",
    "                    #and convert to a categorical object later\n",
    "                    sr_probs_dict: Dict[Tuple[WageState, float], float]={}\n",
    "                    ######################################################################\n",
    "                    # I choose to solve W=w and W=w+1 individually to avoid index out of\n",
    "                    #bound error in later for loop\n",
    "                    \n",
    "                    # if W=w\n",
    "                    if self.W-w==0:\n",
    "                        sr_probs_dict[(WageState(self.W),reward)]=1\n",
    "                    ## if W=w+1        \n",
    "                    elif self.W-w==1:\n",
    "                        sr_probs_dict[(WageState(self.W),reward)]=p_new_job+(1-p_new_job)*(1-poisson.pmf(0,self.a*l))\n",
    "                        sr_probs_dict[(WageState(self.W-1),reward)]=(1-p_new_job)*poisson.pmf(0,self.a*l)\n",
    "                    ## OTW W>w+1, we generalize the solution of prob distribution in a loop \n",
    "                    else:\n",
    "                        # w_nxt=w     p=p(no new job and poisson_x=0)\n",
    "                        sr_probs_dict[(WageState(w),reward)]=(1-p_new_job)*poisson.pmf(0,self.a*l)\n",
    "                        #w_nxt=w+1   p=p(new_job and 0<=x<=1)+p(no new_job and x=1)                       \n",
    "                        sr_probs_dict[(WageState(w+1),reward)]=p_new_job*poisson.cdf(1,self.a*l)+\\\n",
    "                                                                (1-p_new_job)*poisson.pmf(1,self.a*l)\n",
    "                        # All w+2<=w_nxt<W cases, p=p(x=nxt_x) new_job becomes irrelavant.\n",
    "                        for w_nxt in range(w+2,self.W):\n",
    "                            sr_probs_dict[(WageState(w_nxt),reward)]=poisson.pmf(w_nxt-w,self.a*l)\n",
    "                        #w_nxt=W p=p(x>=W-w_nxt)=1-p(x<=W-w_nxt-1)\n",
    "                        sr_probs_dict[(WageState(self.W),reward)]=1-poisson.cdf(self.W-w-1,self.a*l)\n",
    "                    d1[(l,s)] = Categorical(sr_probs_dict)\n",
    "            d[current_state] = d1\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H = 10\n",
    "W = 30\n",
    "a=0.08\n",
    "beta=0.82\n",
    "gamma=0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Iterations: 336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({WageState(wage=1): 1259.6504926227421,\n",
       "  WageState(wage=2): 1340.415028776917,\n",
       "  WageState(wage=3): 1426.3579138423866,\n",
       "  WageState(wage=4): 1517.8111660380023,\n",
       "  WageState(wage=5): 1615.128091467472,\n",
       "  WageState(wage=6): 1718.684649031722,\n",
       "  WageState(wage=7): 1828.8809028830492,\n",
       "  WageState(wage=8): 1946.1425677166949,\n",
       "  WageState(wage=9): 2070.9226507164517,\n",
       "  WageState(wage=10): 2203.703212047008,\n",
       "  WageState(wage=11): 2344.9974308597966,\n",
       "  WageState(wage=12): 2495.3513213053984,\n",
       "  WageState(wage=13): 2655.3256532742776,\n",
       "  WageState(wage=14): 2825.6334066075733,\n",
       "  WageState(wage=15): 3006.9962764014304,\n",
       "  WageState(wage=16): 3199.999895219283,\n",
       "  WageState(wage=17): 3399.9998886704875,\n",
       "  WageState(wage=18): 3599.999882121693,\n",
       "  WageState(wage=19): 3799.9998755728984,\n",
       "  WageState(wage=20): 3999.9998690241036,\n",
       "  WageState(wage=21): 4199.999862475308,\n",
       "  WageState(wage=22): 4399.999855926513,\n",
       "  WageState(wage=23): 4599.9998493777175,\n",
       "  WageState(wage=24): 4799.999842828925,\n",
       "  WageState(wage=25): 4999.99983628013,\n",
       "  WageState(wage=26): 5199.999829731335,\n",
       "  WageState(wage=27): 5399.999823182539,\n",
       "  WageState(wage=28): 5599.9998166337455,\n",
       "  WageState(wage=29): 5799.999810084951,\n",
       "  WageState(wage=30): 5999.999803536155},\n",
       " For State WageState(wage=1):\n",
       "   Do Action (10, 0) with Probability 1.000\n",
       " For State WageState(wage=2):\n",
       "   Do Action (10, 0) with Probability 1.000\n",
       " For State WageState(wage=3):\n",
       "   Do Action (10, 0) with Probability 1.000\n",
       " For State WageState(wage=4):\n",
       "   Do Action (10, 0) with Probability 1.000\n",
       " For State WageState(wage=5):\n",
       "   Do Action (10, 0) with Probability 1.000\n",
       " For State WageState(wage=6):\n",
       "   Do Action (10, 0) with Probability 1.000\n",
       " For State WageState(wage=7):\n",
       "   Do Action (10, 0) with Probability 1.000\n",
       " For State WageState(wage=8):\n",
       "   Do Action (10, 0) with Probability 1.000\n",
       " For State WageState(wage=9):\n",
       "   Do Action (10, 0) with Probability 1.000\n",
       " For State WageState(wage=10):\n",
       "   Do Action (10, 0) with Probability 1.000\n",
       " For State WageState(wage=11):\n",
       "   Do Action (10, 0) with Probability 1.000\n",
       " For State WageState(wage=12):\n",
       "   Do Action (10, 0) with Probability 1.000\n",
       " For State WageState(wage=13):\n",
       "   Do Action (10, 0) with Probability 1.000\n",
       " For State WageState(wage=14):\n",
       "   Do Action (0, 10) with Probability 1.000\n",
       " For State WageState(wage=15):\n",
       "   Do Action (0, 10) with Probability 1.000\n",
       " For State WageState(wage=16):\n",
       "   Do Action (0, 0) with Probability 1.000\n",
       " For State WageState(wage=17):\n",
       "   Do Action (0, 0) with Probability 1.000\n",
       " For State WageState(wage=18):\n",
       "   Do Action (0, 0) with Probability 1.000\n",
       " For State WageState(wage=19):\n",
       "   Do Action (0, 0) with Probability 1.000\n",
       " For State WageState(wage=20):\n",
       "   Do Action (0, 0) with Probability 1.000\n",
       " For State WageState(wage=21):\n",
       "   Do Action (0, 0) with Probability 1.000\n",
       " For State WageState(wage=22):\n",
       "   Do Action (0, 0) with Probability 1.000\n",
       " For State WageState(wage=23):\n",
       "   Do Action (0, 0) with Probability 1.000\n",
       " For State WageState(wage=24):\n",
       "   Do Action (0, 0) with Probability 1.000\n",
       " For State WageState(wage=25):\n",
       "   Do Action (0, 0) with Probability 1.000\n",
       " For State WageState(wage=26):\n",
       "   Do Action (0, 0) with Probability 1.000\n",
       " For State WageState(wage=27):\n",
       "   Do Action (0, 0) with Probability 1.000\n",
       " For State WageState(wage=28):\n",
       "   Do Action (0, 0) with Probability 1.000\n",
       " For State WageState(wage=29):\n",
       "   Do Action (0, 0) with Probability 1.000\n",
       " For State WageState(wage=30):\n",
       "   Do Action (0, 0) with Probability 1.000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new_job_journey=WageMDPFinite(H=H,W=W,a=a,beta=beta)\n",
    "new_job_journey=WageMDPFinite(H=10,W=30,a=0.08,beta=0.82)\n",
    "value_iteration_result(new_job_journey,gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuitive explanantion: \n",
    "When current wage is far below W, we have strong incentive to learn current job(increase L), because it may increase our w by more than 1. However, as w increases, the upside of increasing L is caped by the wage ceiling W. Therefore, increasing S to achieve a more reliable promotion of 1 becomes a better strategy. However, when our current wage w is high enough, the opportunity cost of increasing S becomes larger than possible promotion.For instance,a guranteed one dollar's promotion will only worth 1/(1-0.95)=20 dollars at present. Therefore, we will spend all our time on current job from now on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
