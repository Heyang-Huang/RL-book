{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "import graphviz\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from typing import (Dict, Iterable, Generic, Sequence, Tuple,\n",
    "                    Mapping, Optional, TypeVar)\n",
    "\n",
    "from rl.distribution import (Categorical, Distribution, FiniteDistribution,\n",
    "                             SampledDistribution)\n",
    "\n",
    "S = TypeVar('S')\n",
    "\n",
    "Transition = Mapping[S, Optional[FiniteDistribution[S]]]\n",
    "\n",
    "\n",
    "class MarkovProcess(ABC, Generic[S]):\n",
    "    '''A Markov process with states of type S.\n",
    "    '''\n",
    "    @abstractmethod\n",
    "    def transition(self, state: S) -> Optional[Distribution[S]]:\n",
    "        '''Given a state of the process, returns a distribution of\n",
    "        the next states.  Returning None means we are in a terminal state.\n",
    "        '''\n",
    "\n",
    "    def is_terminal(self, state: S) -> bool:\n",
    "        '''Return whether the given state is a terminal state.\n",
    "        The default implementation of is_terminal calculates a transition\n",
    "        from the current state, so it could be worth overloading this\n",
    "        method if your process has a cheaper way of determing whether\n",
    "        a state is terminal.\n",
    "        '''\n",
    "        return self.transition(state) is None\n",
    "\n",
    "    def simulate(\n",
    "        self,\n",
    "        start_state_distribution: Distribution[S]\n",
    "    ) -> Iterable[S]:\n",
    "        '''Run a simulation trace of this Markov process, generating the\n",
    "        states visited during the trace.\n",
    "        This yields the start state first, then continues yielding\n",
    "        subsequent states forever or until we hit a terminal state.\n",
    "        '''\n",
    "\n",
    "        state: S = start_state_distribution.sample()\n",
    "        while True:\n",
    "            yield state\n",
    "            next_states = self.transition(state)\n",
    "            if next_states is None:\n",
    "                return\n",
    "\n",
    "            state = next_states.sample()\n",
    "\n",
    "    def traces(\n",
    "            self,\n",
    "            start_state_distribution: Distribution[S]\n",
    "    ) -> Iterable[Iterable[S]]:\n",
    "        '''Yield simulation traces (the output of `simulate'), sampling a\n",
    "        start state from the given distribution each time.\n",
    "        '''\n",
    "        while True:\n",
    "            yield self.simulate(start_state_distribution)\n",
    "\n",
    "\n",
    "class FiniteMarkovProcess(MarkovProcess[S]):\n",
    "    '''A Markov Process with a finite state space.\n",
    "    Having a finite state space lets us use tabular methods to work\n",
    "    with the process (ie dynamic programming).\n",
    "    '''\n",
    "\n",
    "    non_terminal_states: Sequence[S]\n",
    "    transition_map: Transition[S]\n",
    "\n",
    "    def __init__(self, transition_map: Transition[S]):\n",
    "        self.non_terminal_states = [s for s, v in transition_map.items()\n",
    "                                    if v is not None]\n",
    "        self.transition_map = transition_map\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "\n",
    "        for s, d in self.transition_map.items():\n",
    "            if d is None:\n",
    "                display += f\"{s} is a Terminal State\\n\"\n",
    "            else:\n",
    "                display += f\"From State {s}:\\n\"\n",
    "                for s1, p in d:\n",
    "                    display += f\"  To State {s1} with Probability {p:.3f}\\n\"\n",
    "\n",
    "        return display\n",
    "\n",
    "    def get_transition_matrix(self) -> np.ndarray:\n",
    "        sz = len(self.non_terminal_states)\n",
    "        mat = np.zeros((sz, sz))\n",
    "\n",
    "        for i, s1 in enumerate(self.non_terminal_states):\n",
    "            for j, s2 in enumerate(self.non_terminal_states):\n",
    "                mat[i, j] = self.transition(s1).probability(s2)\n",
    "\n",
    "        return mat\n",
    "\n",
    "    def transition(self, state: S) -> Optional[FiniteDistribution[S]]:\n",
    "        return self.transition_map[state]\n",
    "\n",
    "    def states(self) -> Iterable[S]:\n",
    "        return self.transition_map.keys()\n",
    "\n",
    "    def get_stationary_distribution(self) -> FiniteDistribution[S]:\n",
    "        eig_vals, eig_vecs = np.linalg.eig(self.get_transition_matrix().T)\n",
    "        index_of_first_unit_eig_val = np.where(\n",
    "            np.abs(eig_vals - 1) < 1e-8)[0][0]\n",
    "        eig_vec_of_unit_eig_val = np.real(\n",
    "            eig_vecs[:, index_of_first_unit_eig_val])\n",
    "        return Categorical({\n",
    "            self.non_terminal_states[i]: ev\n",
    "            for i, ev in enumerate(eig_vec_of_unit_eig_val /\n",
    "                                   sum(eig_vec_of_unit_eig_val))\n",
    "        })\n",
    "\n",
    "    def display_stationary_distribution(self):\n",
    "        pprint({\n",
    "            s: round(p, 3)\n",
    "            for s, p in self.get_stationary_distribution()\n",
    "        })\n",
    "\n",
    "    def generate_image(self) -> graphviz.Digraph:\n",
    "        d = graphviz.Digraph()\n",
    "\n",
    "        for s in self.transition_map.keys():\n",
    "            d.node(str(s))\n",
    "\n",
    "        for s, v in self.transition_map.items():\n",
    "            if v is not None:\n",
    "                for s1, p in v:\n",
    "                    d.edge(str(s), str(s1), label=str(p))\n",
    "\n",
    "        return d\n",
    "\n",
    "\n",
    "# Reward processes\n",
    "@dataclass(frozen=True)\n",
    "class TransitionStep(Generic[S]):\n",
    "    state: S\n",
    "    next_state: S\n",
    "    reward: float\n",
    "\n",
    "    def add_return(self, γ: float, return_: float) -> ReturnStep[S]:\n",
    "        '''Given a γ and the return from 'next_state', this annotates the\n",
    "        transition with a return for 'state'.\n",
    "        '''\n",
    "        return ReturnStep(\n",
    "            self.state,\n",
    "            self.next_state,\n",
    "            self.reward,\n",
    "            return_=self.reward + γ * return_\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ReturnStep(TransitionStep[S]):\n",
    "    return_: float\n",
    "\n",
    "\n",
    "class MarkovRewardProcess(MarkovProcess[S]):\n",
    "    def transition(self, state: S) -> Optional[Distribution[S]]:\n",
    "        '''Transitions the Markov Reward Process, ignoring the generated\n",
    "        reward (which makes this just a normal Markov Process).\n",
    "        '''\n",
    "        distribution = self.transition_reward(state)\n",
    "        if distribution is None:\n",
    "            return None\n",
    "\n",
    "        def next_state(distribution=distribution):\n",
    "            next_s, _ = distribution.sample()\n",
    "            return next_s\n",
    "\n",
    "        return SampledDistribution(next_state)\n",
    "\n",
    "    @abstractmethod\n",
    "    def transition_reward(self, state: S)\\\n",
    "            -> Optional[Distribution[Tuple[S, float]]]:\n",
    "        '''Given a state, returns a distribution of the next state\n",
    "        and reward from transitioning between the states.\n",
    "        '''\n",
    "\n",
    "    def simulate_reward(\n",
    "        self,\n",
    "        start_state_distribution: Distribution[S]\n",
    "    ) -> Iterable[TransitionStep[S]]:\n",
    "        '''Simulate the MRP, yielding an Iterable of\n",
    "        (state, next state, reward) for each sampled transition.\n",
    "        '''\n",
    "\n",
    "        state: S = start_state_distribution.sample()\n",
    "        reward: float = 0.\n",
    "\n",
    "        while True:\n",
    "            next_distribution = self.transition_reward(state)\n",
    "            if next_distribution is None:\n",
    "                return\n",
    "\n",
    "            next_state, reward = next_distribution.sample()\n",
    "            yield TransitionStep(state, next_state, reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    def reward_traces(\n",
    "            self,\n",
    "            start_state_distribution: Distribution[S]\n",
    "    ) -> Iterable[Iterable[TransitionStep[S]]]:\n",
    "        '''Yield simulation traces (the output of `simulate_reward'), sampling\n",
    "        a start state from the given distribution each time.\n",
    "        '''\n",
    "        while True:\n",
    "            yield self.simulate_reward(start_state_distribution)\n",
    "\n",
    "\n",
    "StateReward = FiniteDistribution[Tuple[S, float]]\n",
    "RewardTransition = Mapping[S, Optional[StateReward[S]]]\n",
    "\n",
    "\n",
    "class FiniteMarkovRewardProcess(FiniteMarkovProcess[S],\n",
    "                                MarkovRewardProcess[S]):\n",
    "\n",
    "    transition_reward_map: RewardTransition[S]\n",
    "    reward_function_vec: np.ndarray\n",
    "\n",
    "    def __init__(self, transition_reward_map: RewardTransition[S]):\n",
    "\n",
    "        transition_map: Dict[S, Optional[FiniteDistribution[S]]] = {}\n",
    "\n",
    "        for state, trans in transition_reward_map.items():\n",
    "            if trans is None:\n",
    "                transition_map[state] = None\n",
    "            else:\n",
    "                probabilities: Dict[S, float] = defaultdict(float)\n",
    "                for (next_state, _), probability in trans:\n",
    "                    probabilities[next_state] += probability\n",
    "\n",
    "                transition_map[state] = Categorical(probabilities)\n",
    "\n",
    "        super().__init__(transition_map)\n",
    "\n",
    "        self.transition_reward_map = transition_reward_map\n",
    "\n",
    "        self.reward_function_vec = np.array([\n",
    "            sum(probability * reward for (_, reward), probability in\n",
    "                transition_reward_map[state])\n",
    "            for state in self.non_terminal_states\n",
    "        ])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "        for s, d in self.transition_reward_map.items():\n",
    "            if d is None:\n",
    "                display += f\"{s} is a Terminal State\\n\"\n",
    "            else:\n",
    "                display += f\"From State {s}:\\n\"\n",
    "                for (s1, r), p in d:\n",
    "                    display +=\\\n",
    "                        f\"  To [State {s1} and Reward {r:.3f}]\"\\\n",
    "                        + f\" with Probability {p:.3f}\\n\"\n",
    "        return display\n",
    "\n",
    "    def transition_reward(self, state: S) -> Optional[StateReward[S]]:\n",
    "        return self.transition_reward_map[state]\n",
    "\n",
    "    def get_value_function_vec(self, gamma: float) -> np.ndarray:\n",
    "        return np.linalg.inv(\n",
    "            np.eye(len(self.non_terminal_states)) -\n",
    "            gamma * self.get_transition_matrix()\n",
    "        ).dot(self.reward_function_vec)\n",
    "\n",
    "    def display_reward_function(self):\n",
    "        pprint({\n",
    "            self.non_terminal_states[i]: round(r, 3)\n",
    "            for i, r in enumerate(self.reward_function_vec)\n",
    "        })\n",
    "\n",
    "    def display_value_function(self, gamma: float):\n",
    "        pprint({\n",
    "            self.non_terminal_states[i]: round(v, 3)\n",
    "            for i, v in enumerate(self.get_value_function_vec(gamma))\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
