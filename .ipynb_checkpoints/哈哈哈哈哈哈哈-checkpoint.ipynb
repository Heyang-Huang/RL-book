{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, TypeVar, Callable, Iterator, Sequence, \\\n",
    "    Tuple, Mapping\n",
    "from rl.function_approx import FunctionApprox, Tabular\n",
    "from rl.distribution import Distribution, Choose\n",
    "from rl.markov_process import (MarkovRewardProcess,\n",
    "                               FiniteMarkovRewardProcess, TransitionStep)\n",
    "import itertools\n",
    "import rl.iterate as iterate\n",
    "from rl.returns import returns\n",
    "import rl.monte_carlo as mc\n",
    "from rl.function_approx import learning_rate_schedule\n",
    "import rl.td as td\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from pprint import pprint\n",
    "\n",
    "S = TypeVar('S')\n",
    "\n",
    "\n",
    "def mrp_episodes_stream(\n",
    "    mrp: MarkovRewardProcess[S],\n",
    "    start_state_distribution: Distribution[S]\n",
    ") -> Iterable[Iterable[TransitionStep[S]]]:\n",
    "    return mrp.reward_traces(start_state_distribution)\n",
    "\n",
    "\n",
    "def fmrp_episodes_stream(\n",
    "    fmrp: FiniteMarkovRewardProcess[S]\n",
    ") -> Iterable[Iterable[TransitionStep[S]]]:\n",
    "    return mrp_episodes_stream(fmrp, Choose(set(fmrp.non_terminal_states)))\n",
    "\n",
    "\n",
    "def mc_prediction_equal_wts(\n",
    "    mrp: MarkovRewardProcess[S],\n",
    "    start_state_distribution: Distribution[S],\n",
    "    gamma: float,\n",
    "    tolerance: float,\n",
    "    initial_func_approx: FunctionApprox[S]\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "    episodes: Iterable[Iterable[TransitionStep[S]]] = \\\n",
    "        mrp_episodes_stream(mrp, start_state_distribution)\n",
    "    return mc.mc_prediction(\n",
    "        traces=episodes,\n",
    "        approx_0=initial_func_approx,\n",
    "        γ=gamma,\n",
    "        tolerance=tolerance\n",
    "    )\n",
    "\n",
    "\n",
    "def mc_finite_prediction_equal_wts(\n",
    "    fmrp: FiniteMarkovRewardProcess[S],\n",
    "    gamma: float,\n",
    "    tolerance: float,\n",
    "    initial_vf_dict: Mapping[S, float]\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "    episodes: Iterable[Iterable[TransitionStep[S]]] = \\\n",
    "        fmrp_episodes_stream(fmrp)\n",
    "    return mc.mc_prediction(\n",
    "        traces=episodes,\n",
    "        approx_0=Tabular(values_map=initial_vf_dict),\n",
    "        γ=gamma,\n",
    "        tolerance=tolerance\n",
    "    )\n",
    "\n",
    "\n",
    "def mc_prediction_learning_rate(\n",
    "    mrp: MarkovRewardProcess[S],\n",
    "    start_state_distribution: Distribution[S],\n",
    "    gamma: float,\n",
    "    tolerance: float,\n",
    "    initial_func_approx: FunctionApprox[S]\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "    episodes: Iterable[Iterable[TransitionStep[S]]] = \\\n",
    "        mrp_episodes_stream(mrp, start_state_distribution)\n",
    "    return mc.mc_prediction(\n",
    "        traces=episodes,\n",
    "        approx_0=initial_func_approx,\n",
    "        γ=gamma,\n",
    "        tolerance=tolerance\n",
    "    )\n",
    "\n",
    "\n",
    "def mc_finite_prediction_learning_rate(\n",
    "    fmrp: FiniteMarkovRewardProcess[S],\n",
    "    gamma: float,\n",
    "    tolerance: float,\n",
    "    initial_learning_rate: float,\n",
    "    half_life: float,\n",
    "    exponent: float,\n",
    "    initial_vf_dict: Mapping[S, float]\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "    episodes: Iterable[Iterable[TransitionStep[S]]] = \\\n",
    "        fmrp_episodes_stream(fmrp)\n",
    "    learning_rate_func: Callable[[int], float] = learning_rate_schedule(\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        half_life=half_life,\n",
    "        exponent=exponent\n",
    "    )\n",
    "    return mc.mc_prediction(\n",
    "        traces=episodes,\n",
    "        approx_0=Tabular(\n",
    "            values_map=initial_vf_dict,\n",
    "            count_to_weight_func=learning_rate_func\n",
    "        ),\n",
    "        γ=gamma,\n",
    "        tolerance=tolerance\n",
    "    )\n",
    "\n",
    "\n",
    "def unit_experiences_from_episodes(\n",
    "    episodes: Iterable[Iterable[TransitionStep[S]]],\n",
    "    episode_length: int\n",
    ") -> Iterable[TransitionStep[S]]:\n",
    "    return itertools.chain.from_iterable(\n",
    "        itertools.islice(episode, episode_length) for episode in episodes\n",
    "    )\n",
    "\n",
    "\n",
    "def td_prediction_learning_rate(\n",
    "    mrp: MarkovRewardProcess[S],\n",
    "    start_state_distribution: Distribution[S],\n",
    "    gamma: float,\n",
    "    episode_length: int,\n",
    "    initial_func_approx: FunctionApprox[S]\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "    episodes: Iterable[Iterable[TransitionStep[S]]] = \\\n",
    "        mrp_episodes_stream(mrp, start_state_distribution)\n",
    "    td_experiences: Iterable[TransitionStep[S]] = \\\n",
    "        unit_experiences_from_episodes(\n",
    "            episodes,\n",
    "            episode_length\n",
    "        )\n",
    "    return td.td_prediction(\n",
    "        transitions=td_experiences,\n",
    "        approx_0=initial_func_approx,\n",
    "        γ=gamma\n",
    "    )\n",
    "\n",
    "\n",
    "def td_finite_prediction_learning_rate(\n",
    "    fmrp: FiniteMarkovRewardProcess[S],\n",
    "    gamma: float,\n",
    "    episode_length: int,\n",
    "    initial_learning_rate: float,\n",
    "    half_life: float,\n",
    "    exponent: float,\n",
    "    initial_vf_dict: Mapping[S, float]\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "    episodes: Iterable[Iterable[TransitionStep[S]]] = \\\n",
    "        fmrp_episodes_stream(fmrp)\n",
    "    td_experiences: Iterable[TransitionStep[S]] = \\\n",
    "        unit_experiences_from_episodes(\n",
    "            episodes,\n",
    "            episode_length\n",
    "        )\n",
    "    learning_rate_func: Callable[[int], float] = learning_rate_schedule(\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        half_life=half_life,\n",
    "        exponent=exponent\n",
    "    )\n",
    "    return td.td_prediction(\n",
    "        transitions=td_experiences,\n",
    "        approx_0=Tabular(\n",
    "            values_map=initial_vf_dict,\n",
    "            count_to_weight_func=learning_rate_func\n",
    "        ),\n",
    "        γ=gamma\n",
    "    )\n",
    "\n",
    "\n",
    "def mc_finite_equal_wts_correctness(\n",
    "    fmrp: FiniteMarkovRewardProcess[S],\n",
    "    gamma: float,\n",
    "    tolerance: float,\n",
    "    num_episodes: int,\n",
    "    initial_vf_dict: Mapping[S, float]\n",
    ") -> None:\n",
    "    mc_vfs: Iterator[FunctionApprox[S]] = \\\n",
    "        mc_finite_prediction_equal_wts(\n",
    "            fmrp=fmrp,\n",
    "            gamma=gamma,\n",
    "            tolerance=tolerance,\n",
    "            initial_vf_dict=initial_vf_dict\n",
    "        )\n",
    "    final_mc_vf: FunctionApprox[S] = \\\n",
    "        iterate.last(itertools.islice(mc_vfs, num_episodes))\n",
    "    print(f\"Equal-Weights-MC Value Function with {num_episodes:d} episodes\")\n",
    "    pprint({s: round(final_mc_vf(s), 3) for s in fmrp.non_terminal_states})\n",
    "    print(\"True Value Function\")\n",
    "    fmrp.display_value_function(gamma=gamma)\n",
    "\n",
    "\n",
    "def mc_finite_learning_rate_correctness(\n",
    "    fmrp: FiniteMarkovRewardProcess[S],\n",
    "    gamma: float,\n",
    "    tolerance: float,\n",
    "    num_episodes: int,\n",
    "    initial_learning_rate: float,\n",
    "    half_life: float,\n",
    "    exponent: float,\n",
    "    initial_vf_dict: Mapping[S, float]\n",
    ") -> None:\n",
    "    mc_vfs: Iterator[FunctionApprox[S]] = \\\n",
    "        mc_finite_prediction_learning_rate(\n",
    "            fmrp=fmrp,\n",
    "            gamma=gamma,\n",
    "            tolerance=tolerance,\n",
    "            initial_learning_rate=initial_learning_rate,\n",
    "            half_life=half_life,\n",
    "            exponent=exponent,\n",
    "            initial_vf_dict=initial_vf_dict\n",
    "        )\n",
    "    final_mc_vf: FunctionApprox[S] = \\\n",
    "        iterate.last(itertools.islice(mc_vfs, num_episodes))\n",
    "    print(\"Decaying-Learning-Rate-MC Value Function with \" +\n",
    "          f\"{num_episodes:d} episodes\")\n",
    "    pprint({s: round(final_mc_vf(s), 3) for s in fmrp.non_terminal_states})\n",
    "    print(\"True Value Function\")\n",
    "    fmrp.display_value_function(gamma=gamma)\n",
    "\n",
    "\n",
    "def td_finite_learning_rate_correctness(\n",
    "    fmrp: FiniteMarkovRewardProcess[S],\n",
    "    gamma: float,\n",
    "    episode_length: int,\n",
    "    num_episodes: int,\n",
    "    initial_learning_rate: float,\n",
    "    half_life: float,\n",
    "    exponent: float,\n",
    "    initial_vf_dict: Mapping[S, float]\n",
    ") -> None:\n",
    "    td_vfs: Iterator[FunctionApprox[S]] = \\\n",
    "        td_finite_prediction_learning_rate(\n",
    "            fmrp=fmrp,\n",
    "            gamma=gamma,\n",
    "            episode_length=episode_length,\n",
    "            initial_learning_rate=initial_learning_rate,\n",
    "            half_life=half_life,\n",
    "            exponent=exponent,\n",
    "            initial_vf_dict=initial_vf_dict\n",
    "        )\n",
    "    final_td_vf: FunctionApprox[S] = \\\n",
    "        iterate.last(itertools.islice(td_vfs, episode_length * num_episodes))\n",
    "    print(\"Decaying-Learning-Rate-TD Value Function with \" +\n",
    "          f\"{num_episodes:d} episodes\")\n",
    "    pprint({s: round(final_td_vf(s), 3) for s in fmrp.non_terminal_states})\n",
    "    print(\"True Value Function\")\n",
    "    fmrp.display_value_function(gamma=gamma)\n",
    "\n",
    "\n",
    "def compare_td_and_mc(\n",
    "    fmrp: FiniteMarkovRewardProcess[S],\n",
    "    gamma: float,\n",
    "    mc_episode_length_tol: float,\n",
    "    num_episodes: int,\n",
    "    learning_rates: Sequence[Tuple[float, float, float]],\n",
    "    initial_vf_dict: Mapping[S, float],\n",
    "    plot_batch: int,\n",
    "    plot_start: int\n",
    ") -> None:\n",
    "    true_vf: np.ndarray = fmrp.get_value_function_vec(gamma)\n",
    "    states: Sequence[S] = fmrp.non_terminal_states\n",
    "    colors: Sequence[str] = ['b', 'g', 'r', 'k', 'c', 'm', 'y']\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(11, 7))\n",
    "\n",
    "    for k, (init_lr, half_life, exponent) in enumerate(learning_rates):\n",
    "        mc_funcs_it: Iterator[FunctionApprox[S]] = \\\n",
    "            mc_finite_prediction_learning_rate(\n",
    "                fmrp=fmrp,\n",
    "                gamma=gamma,\n",
    "                tolerance=mc_episode_length_tol,\n",
    "                initial_learning_rate=init_lr,\n",
    "                half_life=half_life,\n",
    "                exponent=exponent,\n",
    "                initial_vf_dict=initial_vf_dict\n",
    "            )\n",
    "        mc_errors = []\n",
    "        batch_mc_errs = []\n",
    "        for i, mc_f in enumerate(itertools.islice(mc_funcs_it, num_episodes)):\n",
    "            batch_mc_errs.append(sqrt(sum(\n",
    "                (mc_f(s) - true_vf[j]) ** 2 for j, s in enumerate(states)\n",
    "            ) / len(states)))\n",
    "            if i % plot_batch == plot_batch - 1:\n",
    "                mc_errors.append(sum(batch_mc_errs) / plot_batch)\n",
    "                batch_mc_errs = []\n",
    "        mc_plot = mc_errors[plot_start:]\n",
    "        label = f\"MC InitRate={init_lr:.3f},HalfLife\" + \\\n",
    "            f\"={half_life:.0f},Exp={exponent:.1f}\"\n",
    "        plt.plot(\n",
    "            range(len(mc_plot)),\n",
    "            mc_plot,\n",
    "            color=colors[k],\n",
    "            linestyle='-',\n",
    "            label=label\n",
    "        )\n",
    "\n",
    "    sample_episodes: int = 1000\n",
    "    td_episode_length: int = int(round(sum(\n",
    "        len(list(returns(\n",
    "            trace=fmrp.simulate_reward(Choose(set(states))),\n",
    "            γ=gamma,\n",
    "            tolerance=mc_episode_length_tol\n",
    "        ))) for _ in range(sample_episodes)\n",
    "    ) / sample_episodes))\n",
    "\n",
    "    for k, (init_lr, half_life, exponent) in enumerate(learning_rates):\n",
    "        td_funcs_it: Iterator[FunctionApprox[S]] = \\\n",
    "            td_finite_prediction_learning_rate(\n",
    "                fmrp=fmrp,\n",
    "                gamma=gamma,\n",
    "                episode_length=td_episode_length,\n",
    "                initial_learning_rate=init_lr,\n",
    "                half_life=half_life,\n",
    "                exponent=exponent,\n",
    "                initial_vf_dict=initial_vf_dict\n",
    "            )\n",
    "        td_errors = []\n",
    "        transitions_batch = plot_batch * td_episode_length\n",
    "        batch_td_errs = []\n",
    "\n",
    "        for i, td_f in enumerate(\n",
    "                itertools.islice(td_funcs_it, num_episodes * td_episode_length)\n",
    "        ):\n",
    "            batch_td_errs.append(sqrt(sum(\n",
    "                (td_f(s) - true_vf[j]) ** 2 for j, s in enumerate(states)\n",
    "            ) / len(states)))\n",
    "            if i % transitions_batch == transitions_batch - 1:\n",
    "                td_errors.append(sum(batch_td_errs) / transitions_batch)\n",
    "                batch_td_errs = []\n",
    "        td_plot = td_errors[plot_start:]\n",
    "        label = f\"TD InitRate={init_lr:.3f},HalfLife\" + \\\n",
    "            f\"={half_life:.0f},Exp={exponent:.1f}\"\n",
    "        plt.plot(\n",
    "            range(len(td_plot)),\n",
    "            td_plot,\n",
    "            color=colors[k],\n",
    "            linestyle='--',\n",
    "            label=label\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Episode Batches\", fontsize=20)\n",
    "    plt.ylabel(\"Value Function RMSE\", fontsize=20)\n",
    "    plt.title(\n",
    "        \"RMSE of MC and TD as function of episode batches\",\n",
    "        fontsize=25\n",
    "    )\n",
    "    plt.grid(True)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
