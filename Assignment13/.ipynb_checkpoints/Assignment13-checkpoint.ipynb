{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "from typing import Mapping,Iterable, Iterator, Tuple, TypeVar,Optional,Callable,Dict\n",
    "from itertools import islice\n",
    "from rl.distribution import Choose\n",
    "from rl.distribution import Categorical\n",
    "from rl.prediction_utils import fmrp_episodes_stream\n",
    "from rl.markov_process import (MarkovRewardProcess,\n",
    "                               FiniteMarkovRewardProcess, TransitionStep)\n",
    "from rl.function_approx import learning_rate_schedule\n",
    "from rl.returns import returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Transition Map\n",
      "------------------\n",
      "From State InventoryState(on_hand=0, on_order=0):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -10.000] with Probability 1.000\n",
      "  With Action 1:\n",
      "    To [State InventoryState(on_hand=0, on_order=1) and Reward -10.000] with Probability 1.000\n",
      "  With Action 2:\n",
      "    To [State InventoryState(on_hand=0, on_order=2) and Reward -10.000] with Probability 1.000\n",
      "From State InventoryState(on_hand=0, on_order=1):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -3.679] with Probability 0.632\n",
      "  With Action 1:\n",
      "    To [State InventoryState(on_hand=1, on_order=1) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=1) and Reward -3.679] with Probability 0.632\n",
      "From State InventoryState(on_hand=0, on_order=2):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=2, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -1.036] with Probability 0.264\n",
      "From State InventoryState(on_hand=1, on_order=0):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -4.679] with Probability 0.632\n",
      "  With Action 1:\n",
      "    To [State InventoryState(on_hand=1, on_order=1) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=1) and Reward -4.679] with Probability 0.632\n",
      "From State InventoryState(on_hand=1, on_order=1):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=2, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -2.036] with Probability 0.264\n",
      "From State InventoryState(on_hand=2, on_order=0):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=2, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -3.036] with Probability 0.264\n",
      "\n",
      "Policy Map\n",
      "----------\n",
      "For State InventoryState(on_hand=0, on_order=0):\n",
      "  Do Action 2 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=1):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=2):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=1):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=2, on_order=0):\n",
      "  Do Action 0 with Probability 1.000\n",
      "\n",
      "Implied MP Transition Map\n",
      "--------------\n",
      "From State InventoryState(on_hand=0, on_order=0):\n",
      "  To State InventoryState(on_hand=0, on_order=2) with Probability 1.000\n",
      "From State InventoryState(on_hand=0, on_order=1):\n",
      "  To State InventoryState(on_hand=1, on_order=1) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=1) with Probability 0.632\n",
      "From State InventoryState(on_hand=0, on_order=2):\n",
      "  To State InventoryState(on_hand=2, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=1, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=0) with Probability 0.264\n",
      "From State InventoryState(on_hand=1, on_order=0):\n",
      "  To State InventoryState(on_hand=1, on_order=1) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=1) with Probability 0.632\n",
      "From State InventoryState(on_hand=1, on_order=1):\n",
      "  To State InventoryState(on_hand=2, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=1, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=0) with Probability 0.264\n",
      "From State InventoryState(on_hand=2, on_order=0):\n",
      "  To State InventoryState(on_hand=2, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=1, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=0) with Probability 0.264\n",
      "\n",
      "Implied MRP Transition Reward Map\n",
      "---------------------\n",
      "From State InventoryState(on_hand=0, on_order=0):\n",
      "  To [State InventoryState(on_hand=0, on_order=2) and Reward -10.000] with Probability 1.000\n",
      "From State InventoryState(on_hand=0, on_order=1):\n",
      "  To [State InventoryState(on_hand=1, on_order=1) and Reward -0.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=1) and Reward -3.679] with Probability 0.632\n",
      "From State InventoryState(on_hand=0, on_order=2):\n",
      "  To [State InventoryState(on_hand=2, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=1, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=0) and Reward -1.036] with Probability 0.264\n",
      "From State InventoryState(on_hand=1, on_order=0):\n",
      "  To [State InventoryState(on_hand=1, on_order=1) and Reward -1.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=1) and Reward -4.679] with Probability 0.632\n",
      "From State InventoryState(on_hand=1, on_order=1):\n",
      "  To [State InventoryState(on_hand=2, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=1, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=0) and Reward -2.036] with Probability 0.264\n",
      "From State InventoryState(on_hand=2, on_order=0):\n",
      "  To [State InventoryState(on_hand=2, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=1, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=0) and Reward -3.036] with Probability 0.264\n",
      "\n",
      "Implied MP Stationary Distribution\n",
      "-----------------------\n",
      "{InventoryState(on_hand=0, on_order=0): 0.117,\n",
      " InventoryState(on_hand=0, on_order=1): 0.279,\n",
      " InventoryState(on_hand=0, on_order=2): 0.117,\n",
      " InventoryState(on_hand=1, on_order=0): 0.162,\n",
      " InventoryState(on_hand=1, on_order=1): 0.162,\n",
      " InventoryState(on_hand=2, on_order=0): 0.162}\n",
      "\n",
      "Implied MRP Reward Function\n",
      "---------------\n",
      "{InventoryState(on_hand=0, on_order=0): -10.0,\n",
      " InventoryState(on_hand=0, on_order=1): -2.325,\n",
      " InventoryState(on_hand=0, on_order=2): -0.274,\n",
      " InventoryState(on_hand=1, on_order=0): -3.325,\n",
      " InventoryState(on_hand=1, on_order=1): -1.274,\n",
      " InventoryState(on_hand=2, on_order=0): -2.274}\n",
      "\n",
      "Implied MRP Value Function\n",
      "--------------\n",
      "{InventoryState(on_hand=0, on_order=0): -35.511,\n",
      " InventoryState(on_hand=0, on_order=1): -27.932,\n",
      " InventoryState(on_hand=0, on_order=2): -28.345,\n",
      " InventoryState(on_hand=1, on_order=0): -28.932,\n",
      " InventoryState(on_hand=1, on_order=1): -29.345,\n",
      " InventoryState(on_hand=2, on_order=0): -30.345}\n",
      "\n",
      "Implied MRP Policy Evaluation Value Function\n",
      "--------------\n",
      "Number of Iterations: 120\n",
      "{InventoryState(on_hand=0, on_order=0): -35.510508591269875,\n",
      " InventoryState(on_hand=0, on_order=1): -27.932164635788453,\n",
      " InventoryState(on_hand=0, on_order=2): -28.345020184031913,\n",
      " InventoryState(on_hand=1, on_order=0): -28.932164635788453,\n",
      " InventoryState(on_hand=1, on_order=1): -29.345020184031913,\n",
      " InventoryState(on_hand=2, on_order=0): -30.345020184031913}\n",
      "\n",
      "MDP Policy Iteration Optimal Value Function and Optimal Policy\n",
      "--------------\n",
      "Number of Iterations: 124\n",
      "Number of Iterations: 120\n",
      "Number of Iterations: 120\n",
      "Number of Iterations: 120\n",
      "Number of Iterations: 3\n",
      "{InventoryState(on_hand=0, on_order=0): -34.89484641847035,\n",
      " InventoryState(on_hand=0, on_order=1): -27.660950868477816,\n",
      " InventoryState(on_hand=0, on_order=2): -27.991890728243845,\n",
      " InventoryState(on_hand=1, on_order=0): -28.660950868477816,\n",
      " InventoryState(on_hand=1, on_order=1): -28.991890728243842,\n",
      " InventoryState(on_hand=2, on_order=0): -29.991890728243842}\n",
      "For State InventoryState(on_hand=0, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=1):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=2):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=1):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=2, on_order=0):\n",
      "  Do Action 0 with Probability 1.000\n",
      "\n",
      "\n",
      "MDP Value Iteration Optimal Value Function and Optimal Policy\n",
      "--------------\n",
      "Number of Iterations: 120\n",
      "{InventoryState(on_hand=0, on_order=0): -34.89484576629397,\n",
      " InventoryState(on_hand=0, on_order=1): -27.66095021630144,\n",
      " InventoryState(on_hand=0, on_order=2): -27.991890076067463,\n",
      " InventoryState(on_hand=1, on_order=0): -28.660950216301437,\n",
      " InventoryState(on_hand=1, on_order=1): -28.991890076067467,\n",
      " InventoryState(on_hand=2, on_order=0): -29.991890076067463}\n",
      "For State InventoryState(on_hand=0, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=1):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=2):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=1):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=2, on_order=0):\n",
      "  Do Action 0 with Probability 1.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import FinitePolicy, StateActionMapping\n",
    "from rl.markov_process import FiniteMarkovProcess, FiniteMarkovRewardProcess\n",
    "from rl.distribution import Categorical, Constant\n",
    "from scipy.stats import poisson\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class InventoryState:\n",
    "    on_hand: int\n",
    "    on_order: int\n",
    "\n",
    "    def inventory_position(self) -> int:\n",
    "        return self.on_hand + self.on_order\n",
    "\n",
    "\n",
    "InvOrderMapping = StateActionMapping[InventoryState, int]\n",
    "\n",
    "\n",
    "class SimpleInventoryMDPCap(FiniteMarkovDecisionProcess[InventoryState, int]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int,\n",
    "        poisson_lambda: float,\n",
    "        holding_cost: float,\n",
    "        stockout_cost: float\n",
    "    ):\n",
    "        self.capacity: int = capacity\n",
    "        self.poisson_lambda: float = poisson_lambda\n",
    "        self.holding_cost: float = holding_cost\n",
    "        self.stockout_cost: float = stockout_cost\n",
    "\n",
    "        self.poisson_distr = poisson(poisson_lambda)\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> InvOrderMapping:\n",
    "        d: Dict[InventoryState, Dict[int, Categorical[Tuple[InventoryState,\n",
    "                                                            float]]]] = {}\n",
    "\n",
    "        for alpha in range(self.capacity + 1):\n",
    "            for beta in range(self.capacity + 1 - alpha):\n",
    "                state: InventoryState = InventoryState(alpha, beta)\n",
    "                ip: int = state.inventory_position()\n",
    "                base_reward: float = - self.holding_cost * alpha\n",
    "                d1: Dict[int, Categorical[Tuple[InventoryState, float]]] = {}\n",
    "\n",
    "                for order in range(self.capacity - ip + 1):\n",
    "                    sr_probs_dict: Dict[Tuple[InventoryState, float], float] =\\\n",
    "                        {(InventoryState(ip - i, order), base_reward):\n",
    "                         self.poisson_distr.pmf(i) for i in range(ip)}\n",
    "\n",
    "                    probability: float = 1 - self.poisson_distr.cdf(ip - 1)\n",
    "                    reward: float = base_reward - self.stockout_cost *\\\n",
    "                        (probability * (self.poisson_lambda - ip) +\n",
    "                         ip * self.poisson_distr.pmf(ip))\n",
    "                    sr_probs_dict[(InventoryState(0, order), reward)] = \\\n",
    "                        probability\n",
    "                    d1[order] = Categorical(sr_probs_dict)\n",
    "\n",
    "                d[state] = d1\n",
    "        return d\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from pprint import pprint\n",
    "\n",
    "    user_capacity = 2\n",
    "    user_poisson_lambda = 1.0\n",
    "    user_holding_cost = 1.0\n",
    "    user_stockout_cost = 10.0\n",
    "\n",
    "    user_gamma = 0.9\n",
    "\n",
    "    si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] =\\\n",
    "        SimpleInventoryMDPCap(\n",
    "            capacity=user_capacity,\n",
    "            poisson_lambda=user_poisson_lambda,\n",
    "            holding_cost=user_holding_cost,\n",
    "            stockout_cost=user_stockout_cost\n",
    "        )\n",
    "\n",
    "    print(\"MDP Transition Map\")\n",
    "    print(\"------------------\")\n",
    "    print(si_mdp)\n",
    "\n",
    "    fdp: FinitePolicy[InventoryState, int] = FinitePolicy(\n",
    "        {InventoryState(alpha, beta):\n",
    "         Constant(user_capacity - (alpha + beta)) for alpha in\n",
    "         range(user_capacity + 1) for beta in range(user_capacity + 1 - alpha)}\n",
    "    )\n",
    "\n",
    "    print(\"Policy Map\")\n",
    "    print(\"----------\")\n",
    "    print(fdp)\n",
    "\n",
    "    implied_mrp: FiniteMarkovRewardProcess[InventoryState] =\\\n",
    "        si_mdp.apply_finite_policy(fdp)\n",
    "    print(\"Implied MP Transition Map\")\n",
    "    print(\"--------------\")\n",
    "    print(FiniteMarkovProcess(implied_mrp.transition_map))\n",
    "\n",
    "    print(\"Implied MRP Transition Reward Map\")\n",
    "    print(\"---------------------\")\n",
    "    print(implied_mrp)\n",
    "\n",
    "    print(\"Implied MP Stationary Distribution\")\n",
    "    print(\"-----------------------\")\n",
    "    implied_mrp.display_stationary_distribution()\n",
    "    print()\n",
    "\n",
    "    print(\"Implied MRP Reward Function\")\n",
    "    print(\"---------------\")\n",
    "    implied_mrp.display_reward_function()\n",
    "    print()\n",
    "\n",
    "    print(\"Implied MRP Value Function\")\n",
    "    print(\"--------------\")\n",
    "    implied_mrp.display_value_function(gamma=user_gamma)\n",
    "    print()\n",
    "\n",
    "    from rl.dynamic_programming import evaluate_mrp_result\n",
    "    from rl.dynamic_programming import policy_iteration_result\n",
    "    from rl.dynamic_programming import value_iteration_result\n",
    "\n",
    "    print(\"Implied MRP Policy Evaluation Value Function\")\n",
    "    print(\"--------------\")\n",
    "    pprint(evaluate_mrp_result(implied_mrp, gamma=user_gamma))\n",
    "    print()\n",
    "\n",
    "    print(\"MDP Policy Iteration Optimal Value Function and Optimal Policy\")\n",
    "    print(\"--------------\")\n",
    "    opt_vf_pi, opt_policy_pi = policy_iteration_result(\n",
    "        si_mdp,\n",
    "        gamma=user_gamma\n",
    "    )\n",
    "    pprint(opt_vf_pi)\n",
    "    print(opt_policy_pi)\n",
    "    print()\n",
    "\n",
    "    print(\"MDP Value Iteration Optimal Value Function and Optimal Policy\")\n",
    "    print(\"--------------\")\n",
    "    opt_vf_vi, opt_policy_vi = value_iteration_result(si_mdp, gamma=user_gamma)\n",
    "    pprint(opt_vf_vi)\n",
    "    print(opt_policy_vi)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p1 p2 p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Sequence, Set, Mapping, Dict, Callable, Optional\n",
    "from dataclasses import dataclass\n",
    "from operator import itemgetter\n",
    "from rl.distribution import Categorical, Choose, Constant\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import StateActionMapping\n",
    "from rl.markov_decision_process import FinitePolicy\n",
    "from rl.dynamic_programming import value_iteration_result, V\n",
    "\n",
    "class SimpleInventoryMDPCap_RL(FiniteMarkovDecisionProcess[InventoryState, int]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int,\n",
    "        poisson_lambda: float,\n",
    "        holding_cost: float,\n",
    "        stockout_cost: float\n",
    "    ):\n",
    "        self.capacity: int = capacity\n",
    "        self.poisson_lambda: float = poisson_lambda\n",
    "        self.holding_cost: float = holding_cost\n",
    "        self.stockout_cost: float = stockout_cost\n",
    "\n",
    "        self.poisson_distr = poisson(poisson_lambda)\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> InvOrderMapping:\n",
    "        d: Dict[InventoryState, Dict[int, Categorical[Tuple[InventoryState,\n",
    "                                                            float]]]] = {}\n",
    "\n",
    "        for alpha in range(self.capacity + 1):\n",
    "            for beta in range(self.capacity + 1 - alpha):\n",
    "                state: InventoryState = InventoryState(alpha, beta)\n",
    "                ip: int = state.inventory_position()\n",
    "                base_reward: float = - self.holding_cost * alpha\n",
    "                d1: Dict[int, Categorical[Tuple[InventoryState, float]]] = {}\n",
    "\n",
    "                for order in range(self.capacity - ip + 1):\n",
    "                    sr_probs_dict: Dict[Tuple[InventoryState, float], float] =\\\n",
    "                        {(InventoryState(ip - i, order), base_reward):\n",
    "                         self.poisson_distr.pmf(i) for i in range(ip)}\n",
    "\n",
    "                    probability: float = 1 - self.poisson_distr.cdf(ip - 1)\n",
    "                    reward: float = base_reward - self.stockout_cost *\\\n",
    "                        (probability * (self.poisson_lambda - ip) +\n",
    "                         ip * self.poisson_distr.pmf(ip))\n",
    "                    sr_probs_dict[(InventoryState(0, order), reward)] = \\\n",
    "                        probability\n",
    "                    d1[order] = Categorical(sr_probs_dict)\n",
    "\n",
    "                d[state] = d1\n",
    "        return d\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def epsilon_greedy_action(\n",
    "        nt_state: InventoryState,\n",
    "        q: Mapping[InventoryState, Mapping[int, float]],\n",
    "        epsilon: float\n",
    "    ) -> int:\n",
    "        '''\n",
    "        given a non-terminal state, a Q-Value Function (in the form of a\n",
    "        {state: {action: Expected Return}} dictionary) and epislon, return\n",
    "        an action sampled from the probability distribution implied by an\n",
    "        epsilon-greedy policy that is derived from the Q-Value Function.\n",
    "        '''\n",
    "        action_values: Mapping[int, float] = q[nt_state]\n",
    "        greedy_action: Move = max(action_values.items(), key=itemgetter(1))[0]\n",
    "        return Categorical(\n",
    "            {a: epsilon / len(action_values) +\n",
    "             (1 - epsilon if a == greedy_action else 0.)\n",
    "             for a in action_values}\n",
    "        ).sample()\n",
    "\n",
    "    def get_states_actions_dict(self) -> Mapping[InventoryState, Optional[Set[int]]]:\n",
    "        '''\n",
    "        Returns a dictionary whose keys are the states and the corresponding\n",
    "        values are the set of actions for the state (if the key is a\n",
    "        non-terminal state) or is None if the state is a terminal state.\n",
    "        '''\n",
    "        d1: Mapping[InventoryState, Optional[Set[int]]] = \\\n",
    "            {s: {a for a, _ in self.get_actions_and_next_states(s)}\n",
    "             for s in self.get_all_nt_states()}\n",
    "        d2: Mapping[InventoryState, Optional[Set[int]]] = \\\n",
    "            {s: None for s in self.terminals}\n",
    "        return {**d1, **d2}\n",
    "\n",
    "    def get_sarsa_vf_and_policy(\n",
    "        self,\n",
    "        states_actions_dict: Mapping[InventoryState, Optional[Set[int]]],\n",
    "        sample_func: Callable[[InventoryState, int], Tuple[InventoryState, float]],\n",
    "        episodes: int = 10000,\n",
    "        step_size: float = 0.01\n",
    "    ) -> Tuple[V[InventoryState], FinitePolicy[InventoryState, int]]:\n",
    "        '''\n",
    "        states_actions_dict gives us the set of possible moves from\n",
    "        a non-block cell.\n",
    "        sample_func is a function with two inputs: state and action,\n",
    "        and with output as a sampled pair of (next_state, reward).\n",
    "        '''\n",
    "        q: Dict[InventoryState, Dict[int, float]] = \\\n",
    "            {s: {a: 0. for a in actions} for s, actions in\n",
    "             states_actions_dict.items() if actions is not None}\n",
    "        nt_states = {s for s in q}\n",
    "        uniform_states: Choose[InventoryState] = Choose(nt_states)\n",
    "        for episode_num in range(episodes):\n",
    "            epsilon: float = 1.0 / (episode_num + 1)\n",
    "            state: InventoryState = uniform_states.sample()\n",
    "\n",
    "            # Since we are at a non-terminating state, we can always get an action wrt epsilon greedy policy\n",
    "            sampled_a=self.epsilon_greedy_action(nt_state=state,q=q,epsilon=epsilon)\n",
    "            while True:\n",
    "                # sample the next state and reward pair\n",
    "                nxt_state,r=sample_func(state,sampled_a)\n",
    "                # if next state is T,we update with Q(s_t+1,A_t+1)=0 and stop this episode\n",
    "                if nxt_state in self.terminals:\n",
    "                    q[state][sampled_a]=q[state][sampled_a]+step_size*(r-q[state][sampled_a])\n",
    "                    break\n",
    "                # Else update q table and continue\n",
    "                sampled_nxt_a=self.epsilon_greedy_action(nt_state=nxt_state,q=q,epsilon=epsilon)\n",
    "                q[state][sampled_a]=q[state][sampled_a]+step_size*(r+q[nxt_state][sampled_nxt_a]-q[state][sampled_a])\n",
    "                state=nxt_state\n",
    "                sampled_a=sampled_nxt_a\n",
    "        vf_dict: V[InventoryState] = {s: max(d.values()) for s, d in q.items()}\n",
    "        policy: FinitePolicy[InventoryState, Move] = FinitePolicy(\n",
    "            {s: Constant(max(d.items(), key=itemgetter(1))[0])\n",
    "             for s, d in q.items()}\n",
    "        )\n",
    "        return (vf_dict, policy)\n",
    "\n",
    "    def get_q_learning_vf_and_policy(\n",
    "        self,\n",
    "        states_actions_dict: Mapping[InventoryState, Optional[Set[int]]],\n",
    "        sample_func: Callable[[InventoryState, int], Tuple[InventoryState, float]],\n",
    "        episodes: int = 10000,\n",
    "        step_size: float = 0.01,\n",
    "        epsilon: float = 0.1\n",
    "    ) -> Tuple[V[int], FinitePolicy[InventoryState, int]]:\n",
    "        '''\n",
    "        states_actions_dict gives us the set of possible moves from\n",
    "        a non-block cell.\n",
    "        sample_func is a function with two inputs: state and action,\n",
    "        and with output as a sampled pair of (next_state, reward).\n",
    "        '''\n",
    "        q: Dict[int, Dict[int, float]] = \\\n",
    "            {s: {a: 0. for a in actions} for s, actions in\n",
    "             states_actions_dict.items() if actions is not None}\n",
    "        nt_states = {s for s in q}\n",
    "        uniform_states: Choose[InventoryState] = Choose(nt_states)\n",
    "        for episode_num in range(episodes):\n",
    "            state: InventoryState = uniform_states.sample()\n",
    "                \n",
    "        # Since we are at a non-terminating state, we can always get an action wrt epsilon greedy policy\n",
    "            sampled_a=self.epsilon_greedy_action(nt_state=state,q=q,epsilon=epsilon)\n",
    "            while True:\n",
    "                # sample the next state and reward pair\n",
    "                nxt_state,r=sample_func(state,sampled_a)\n",
    "                # if next state is T,we update with Q(s_t+1,A_t+1)=0 and stop this episode\n",
    "                if nxt_state in self.terminals:\n",
    "                    q[state][sampled_a]=q[state][sampled_a]+step_size*(r-q[state][sampled_a])\n",
    "                    break\n",
    "                # Else we take next action from a greedy policy(epsilon=0) and update Q table\n",
    "                sampled_nxt_a=self.epsilon_greedy_action(nt_state=nxt_state,q=q,epsilon=0)\n",
    "                q[state][sampled_a]=q[state][sampled_a]+step_size*(r+q[nxt_state][sampled_nxt_a]-q[state][sampled_a])\n",
    "                state=nxt_state\n",
    "                sampled_a=sampled_nxt_a   \n",
    "            \n",
    "        vf_dict: V[InventoryState] = {s: max(d.values()) for s, d in q.items()}\n",
    "        policy: FinitePolicy[InventoryState, int] = FinitePolicy(\n",
    "            {s: Constant(max(d.items(), key=itemgetter(1))[0])\n",
    "             for s, d in q.items()}\n",
    "        )\n",
    "        return (vf_dict, policy)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
