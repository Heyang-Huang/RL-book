{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Generator,Callable,Optional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "from rl.markov_process import MarkovRewardProcess,FiniteMarkovProcess,MarkovProcess\n",
    "from rl.markov_process import FiniteMarkovRewardProcess\n",
    "from rl.markov_process import Transition,TransitionStep,ReturnStep\n",
    "from rl.markov_process import RewardTransition\n",
    "from rl.distribution import Constant,Categorical,SampledDistribution,Distribution\n",
    "from rl.gen_utils.common_funcs import get_logistic_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p1 p2\n",
    "\n",
    "### We incorporate MC and TD Tabular in a single class and instead implement two methods: mc_tabular td_tabular "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rl.distribution'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3ec450beeb8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdataclasses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataclass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mChoose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConstant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkov_decision_process\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFiniteMarkovDecisionProcess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkov_decision_process\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStateActionMapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rl.distribution'"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, Sequence, Set, Mapping, Dict, Callable, Optional\n",
    "from dataclasses import dataclass\n",
    "from operator import itemgetter\n",
    "from rl.distribution import Categorical, Choose, Constant\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import StateActionMapping\n",
    "from rl.markov_decision_process import FinitePolicy\n",
    "from rl.dynamic_programming import value_iteration_result, V\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class WindyGrid:\n",
    "\n",
    "    rows: int  # number of grid rows\n",
    "    columns: int  # number of grid columns\n",
    "    blocks: CellSet  # coordinates of block cells\n",
    "    terminals: CellSet  # coordinates of goal cells\n",
    "    wind: WindSpec  # spec of vertical random wind for the columns\n",
    "    bump_cost: float  # cost of bumping against block or boundary\n",
    "\n",
    "    def validate_spec(self) -> bool:\n",
    "        b1 = self.rows >= 2\n",
    "        b2 = self.columns >= 2\n",
    "        b3 = all(0 <= r < self.rows and 0 <= c < self.columns\n",
    "                 for r, c in self.blocks)\n",
    "        b4 = len(self.terminals) >= 1\n",
    "        b5 = all(0 <= r < self.rows and 0 <= c < self.columns and\n",
    "                 (r, c) not in self.blocks for r, c in self.terminals)\n",
    "        b6 = len(self.wind) == self.columns\n",
    "        b7 = all(0. <= p1 <= 1. and 0. <= p2 <= 1. and p1 + p2 <= 1.\n",
    "                 for p1, p2 in self.wind)\n",
    "        b8 = self.bump_cost > 0.\n",
    "        return all([b1, b2, b3, b4, b5, b6, b7, b8])\n",
    "\n",
    "    def print_wind_and_bumps(self) -> None:\n",
    "        for i, (d, u) in enumerate(self.wind):\n",
    "            print(f\"Column {i:d}: Down Prob = {d:.2f}, Up Prob = {u:.2f}\")\n",
    "        print(f\"Bump Cost = {self.bump_cost:.2f}\")\n",
    "        print()\n",
    "\n",
    "    @staticmethod\n",
    "    def add_tuples(a: Cell, b: Cell) -> Cell:\n",
    "        return a[0] + b[0], a[1] + b[1]\n",
    "\n",
    "    def is_valid_state(self, cell: Cell) -> bool:\n",
    "        '''\n",
    "        checks if a cell is a valid state of the MDP\n",
    "        '''\n",
    "        return 0 <= cell[0] < self.rows and 0 <= cell[1] < self.columns \\\n",
    "            and cell not in self.blocks\n",
    "\n",
    "    def get_all_nt_states(self) -> CellSet:\n",
    "        '''\n",
    "        returns all the non-terminal states\n",
    "        '''\n",
    "        return {(i, j) for i in range(self.rows) for j in range(self.columns)\n",
    "                if (i, j) not in set.union(self.blocks, self.terminals)}\n",
    "\n",
    "    def get_actions_and_next_states(self, nt_state: Cell) \\\n",
    "            -> Set[Tuple[Move, Cell]]:\n",
    "        '''\n",
    "        given a non-terminal state, returns the set of all possible\n",
    "        (action, next_state) pairs\n",
    "        '''\n",
    "        temp: Set[Tuple[Move, Cell]] = {(a, WindyGrid.add_tuples(nt_state, a))\n",
    "                                        for a in possible_moves}\n",
    "        return {(a, s) for a, s in temp if self.is_valid_state(s)}\n",
    "\n",
    "    def get_transition_probabilities(self, nt_state: Cell) \\\n",
    "            -> Mapping[Move, Categorical[Tuple[Cell, float]]]:\n",
    "        '''\n",
    "        given a non-terminal state, return a dictionary whose\n",
    "        keys are the valid actions (moves) from the given state\n",
    "        and the corresponding values are the associated probabilities\n",
    "        (following that move) of the (next_state, reward) pairs.\n",
    "        The probabilities are determined from the wind probabilities\n",
    "        of the column one is in after the move. Note that if one moves\n",
    "        to a goal cell (terminal state), then one ends up in that\n",
    "        goal cell with 100% probability (i.e., no wind exposure in a\n",
    "        goal cell).\n",
    "        '''\n",
    "        d: Dict[Move, Categorical[Tuple[Cell, float]]] = {}\n",
    "        for a, (r, c) in self.get_actions_and_next_states(nt_state):\n",
    "            if (r, c) in self.terminals:\n",
    "                d[a] = Categorical({((r, c), -1.): 1.})\n",
    "            else:\n",
    "                d1={}\n",
    "                up_valid=self.is_valid_state((r+1,c))\n",
    "                down_valid=self.is_valid_state((r-1,c))\n",
    "                #both up and down valid#\n",
    "                if up_valid and down_valid:\n",
    "                    d1[((r,c),-1)]=1-self.wind[c][0]-self.wind[c][1]\n",
    "                    d1[((r-1,c),-1)]=self.wind[c][0]\n",
    "                    d1[((r+1,c),-1)]=self.wind[c][1]\n",
    "                #only up valid#\n",
    "                elif up_valid:\n",
    "                    d1[((r,c),-1)]=1-self.wind[c][0]-self.wind[c][1]\n",
    "                    d1[((r,c),-1-self.bump_cost)]=self.wind[c][0]\n",
    "                    d1[((r+1,c),-1)]=self.wind[c][1]\n",
    "                #only down valid#\n",
    "                elif down_valid:\n",
    "                    d1[((r,c),-1)]=1-self.wind[c][0]-self.wind[c][1]\n",
    "                    d1[((r,c),-1-self.bump_cost)]=self.wind[c][1]\n",
    "                    d1[((r-1,c),-1)]=self.wind[c][0]\n",
    "                #neither up nor down valid#\n",
    "                else:\n",
    "                    d1[((r,c),-1)]=1-self.wind[c][0]-self.wind[c][1]\n",
    "                    d1[((r,c),-1-self.bump_cost)]=self.wind[c][0]+self.wind[c][1]\n",
    "                d[a]=Categorical(d1)\n",
    "        return d\n",
    "\n",
    "    def get_finite_mdp(self) -> FiniteMarkovDecisionProcess[Cell, Move]:\n",
    "        '''\n",
    "        returns the FiniteMarkovDecision object for this windy grid problem\n",
    "        '''\n",
    "        d1: StateActionMapping[Cell, Move] = \\\n",
    "            {s: self.get_transition_probabilities(s) for s in\n",
    "             self.get_all_nt_states()}\n",
    "        d2: StateActionMapping[Cell, Move] = {s: None for s in self.terminals}\n",
    "        return FiniteMarkovDecisionProcess({**d1, **d2})\n",
    "\n",
    "    def get_vi_vf_and_policy(self) -> Tuple[V[Cell], FinitePolicy[Cell, Move]]:\n",
    "        '''\n",
    "        Performs the Value Iteration DP algorithm returning the\n",
    "        Optimal Value Function (as a V[Cell]) and the Optimal Policy\n",
    "        (as a FinitePolicy[Cell, Move])\n",
    "        '''\n",
    "        return value_iteration_result(self.get_finite_mdp(), gamma=1.)\n",
    "\n",
    "    @staticmethod\n",
    "    def epsilon_greedy_action(\n",
    "        nt_state: Cell,\n",
    "        q: Mapping[Cell, Mapping[Move, float]],\n",
    "        epsilon: float\n",
    "    ) -> Move:\n",
    "        '''\n",
    "        given a non-terminal state, a Q-Value Function (in the form of a\n",
    "        {state: {action: Expected Return}} dictionary) and epislon, return\n",
    "        an action sampled from the probability distribution implied by an\n",
    "        epsilon-greedy policy that is derived from the Q-Value Function.\n",
    "        '''\n",
    "        action_values: Mapping[Move, float] = q[nt_state]\n",
    "        greedy_action: Move = max(action_values.items(), key=itemgetter(1))[0]\n",
    "        return Categorical(\n",
    "            {a: epsilon / len(action_values) +\n",
    "             (1 - epsilon if a == greedy_action else 0.)\n",
    "             for a in action_values}\n",
    "        ).sample()\n",
    "\n",
    "    def get_states_actions_dict(self) -> Mapping[Cell, Optional[Set[Move]]]:\n",
    "        '''\n",
    "        Returns a dictionary whose keys are the states and the corresponding\n",
    "        values are the set of actions for the state (if the key is a\n",
    "        non-terminal state) or is None if the state is a terminal state.\n",
    "        '''\n",
    "        d1: Mapping[Cell, Optional[Set[Move]]] = \\\n",
    "            {s: {a for a, _ in self.get_actions_and_next_states(s)}\n",
    "             for s in self.get_all_nt_states()}\n",
    "        d2: Mapping[Cell, Optional[Set[Move]]] = \\\n",
    "            {s: None for s in self.terminals}\n",
    "        return {**d1, **d2}\n",
    "\n",
    "    def get_sarsa_vf_and_policy(\n",
    "        self,\n",
    "        states_actions_dict: Mapping[Cell, Optional[Set[Move]]],\n",
    "        sample_func: Callable[[Cell, Move], Tuple[Cell, float]],\n",
    "        episodes: int = 10000,\n",
    "        step_size: float = 0.01\n",
    "    ) -> Tuple[V[Cell], FinitePolicy[Cell, Move]]:\n",
    "        '''\n",
    "        states_actions_dict gives us the set of possible moves from\n",
    "        a non-block cell.\n",
    "        sample_func is a function with two inputs: state and action,\n",
    "        and with output as a sampled pair of (next_state, reward).\n",
    "        '''\n",
    "        q: Dict[Cell, Dict[Move, float]] = \\\n",
    "            {s: {a: 0. for a in actions} for s, actions in\n",
    "             states_actions_dict.items() if actions is not None}\n",
    "        nt_states: CellSet = {s for s in q}\n",
    "        uniform_states: Choose[Cell] = Choose(nt_states)\n",
    "        for episode_num in range(episodes):\n",
    "            epsilon: float = 1.0 / (episode_num + 1)\n",
    "            state: Cell = uniform_states.sample()\n",
    "            '''\n",
    "            write your code here\n",
    "            update the dictionary q initialized above according\n",
    "            to the SARSA algorithm's Q-Value Function updates.\n",
    "            '''\n",
    "            # Since we are at a non-terminating state, we can always get an action wrt epsilon greedy policy\n",
    "            sampled_a=self.epsilon_greedy_action(nt_state=state,q=q,epsilon=epsilon)\n",
    "            while True:\n",
    "                # sample the next state and reward pair\n",
    "                nxt_state,r=sample_func(state,sampled_a)\n",
    "                # if next state is T,we update with Q(s_t+1,A_t+1)=0 and stop this episode\n",
    "                if nxt_state in self.terminals:\n",
    "                    q[state][sampled_a]=q[state][sampled_a]+step_size*(r-q[state][sampled_a])\n",
    "                    break\n",
    "                # Else update q table and continue\n",
    "                sampled_nxt_a=self.epsilon_greedy_action(nt_state=nxt_state,q=q,epsilon=epsilon)\n",
    "                q[state][sampled_a]=q[state][sampled_a]+step_size*(r+q[nxt_state][sampled_nxt_a]-q[state][sampled_a])\n",
    "                state=nxt_state\n",
    "                sampled_a=sampled_nxt_a\n",
    "        vf_dict: V[Cell] = {s: max(d.values()) for s, d in q.items()}\n",
    "        policy: FinitePolicy[Cell, Move] = FinitePolicy(\n",
    "            {s: Constant(max(d.items(), key=itemgetter(1))[0])\n",
    "             for s, d in q.items()}\n",
    "        )\n",
    "        return (vf_dict, policy)\n",
    "\n",
    "    def get_q_learning_vf_and_policy(\n",
    "        self,\n",
    "        states_actions_dict: Mapping[Cell, Optional[Set[Move]]],\n",
    "        sample_func: Callable[[Cell, Move], Tuple[Cell, float]],\n",
    "        episodes: int = 10000,\n",
    "        step_size: float = 0.01,\n",
    "        epsilon: float = 0.1\n",
    "    ) -> Tuple[V[Cell], FinitePolicy[Cell, Move]]:\n",
    "        '''\n",
    "        states_actions_dict gives us the set of possible moves from\n",
    "        a non-block cell.\n",
    "        sample_func is a function with two inputs: state and action,\n",
    "        and with output as a sampled pair of (next_state, reward).\n",
    "        '''\n",
    "        q: Dict[Cell, Dict[Move, float]] = \\\n",
    "            {s: {a: 0. for a in actions} for s, actions in\n",
    "             states_actions_dict.items() if actions is not None}\n",
    "        nt_states: CellSet = {s for s in q}\n",
    "        uniform_states: Choose[Cell] = Choose(nt_states)\n",
    "        for episode_num in range(episodes):\n",
    "            state: Cell = uniform_states.sample()\n",
    "            '''\n",
    "            write your code here\n",
    "            update the dictionary q initialized above according\n",
    "            to the Q-learning algorithm's Q-Value Function updates.\n",
    "            '''\n",
    "        # Since we are at a non-terminating state, we can always get an action wrt epsilon greedy policy\n",
    "            sampled_a=self.epsilon_greedy_action(nt_state=state,q=q,epsilon=epsilon)\n",
    "            while True:\n",
    "                # sample the next state and reward pair\n",
    "                nxt_state,r=sample_func(state,sampled_a)\n",
    "                # if next state is T,we update with Q(s_t+1,A_t+1)=0 and stop this episode\n",
    "                if nxt_state in self.terminals:\n",
    "                    q[state][sampled_a]=q[state][sampled_a]+step_size*(r-q[state][sampled_a])\n",
    "                    break\n",
    "                # Else we take next action from a greedy policy(epsilon=0) and update Q table\n",
    "                sampled_nxt_a=self.epsilon_greedy_action(nt_state=nxt_state,q=q,epsilon=0)\n",
    "                q[state][sampled_a]=q[state][sampled_a]+step_size*(r+q[nxt_state][sampled_nxt_a]-q[state][sampled_a])\n",
    "                state=nxt_state\n",
    "                sampled_a=sampled_nxt_a   \n",
    "            \n",
    "        vf_dict: V[Cell] = {s: max(d.values()) for s, d in q.items()}\n",
    "        policy: FinitePolicy[Cell, Move] = FinitePolicy(\n",
    "            {s: Constant(max(d.items(), key=itemgetter(1))[0])\n",
    "             for s, d in q.items()}\n",
    "        )\n",
    "        return (vf_dict, policy)\n",
    "\n",
    "    def print_vf_and_policy(\n",
    "        self,\n",
    "        vf_dict: V[Cell],\n",
    "        policy: FinitePolicy[Cell, Move]\n",
    "    ) -> None:\n",
    "        display = \"%5.2f\"\n",
    "        display1 = \"%5d\"\n",
    "        vf_full_dict = {\n",
    "            **{s: display % -v for s, v in vf_dict.items()},\n",
    "            **{s: display % 0.0 for s in self.terminals},\n",
    "            **{s: 'X' * 5 for s in self.blocks}\n",
    "        }\n",
    "        print(\"   \" + \" \".join([display1 % j for j in range(self.columns)]))\n",
    "        for i in range(self.rows - 1, -1, -1):\n",
    "            print(\"%2d \" % i + \" \".join(vf_full_dict[(i, j)]\n",
    "                                        for j in range(self.columns)))\n",
    "        print()\n",
    "        pol_full_dict = {\n",
    "            **{s: possible_moves[policy.act(s).value]\n",
    "               for s in self.get_all_nt_states()},\n",
    "            **{s: 'T' for s in self.terminals},\n",
    "            **{s: 'X' for s in self.blocks}\n",
    "        }\n",
    "        print(\"   \" + \" \".join([\"%2d\" % j for j in range(self.columns)]))\n",
    "        for i in range(self.rows - 1, -1, -1):\n",
    "            print(\"%2d  \" % i + \"  \".join(pol_full_dict[(i, j)]\n",
    "                                          for j in range(self.columns)))\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
