{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "from rl.function_approx import *\n",
    "from typing import Callable, Iterable, Iterator, TypeVar, Tuple\n",
    "\n",
    "from rl.function_approx import FunctionApprox\n",
    "import rl.markov_process as mp\n",
    "import rl.markov_decision_process as mdp\n",
    "import rl.iterate as iterate\n",
    "\n",
    "import unittest\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "from typing import cast, Iterable, Iterator, Optional, Tuple\n",
    "\n",
    "from rl.distribution import Categorical, Choose\n",
    "from rl.function_approx import Tabular\n",
    "import rl.iterate as iterate\n",
    "from rl.markov_process import FiniteMarkovRewardProcess\n",
    "import rl.markov_process as mp\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "import rl.markov_decision_process as mdp\n",
    "import rl.td as td\n",
    "\n",
    "S = TypeVar('S')\n",
    "\n",
    "A = TypeVar('A')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlipFlop(FiniteMarkovRewardProcess[bool]):\n",
    "    '''A version of FlipFlop implemented with the FiniteMarkovProcess\n",
    "    machinery.\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, p: float):\n",
    "        transition_reward_map = {\n",
    "            b: Categorical({(not b, 2.0): p, (b, 1.0): 1 - p})\n",
    "            for b in (True, False)\n",
    "        }\n",
    "        super().__init__(transition_reward_map)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_mdp=FiniteMarkovDecisionProcess({\n",
    "            True: {\n",
    "                True: Categorical({(True, 1.0): 0.7, (False, 2.0): 0.3}),\n",
    "                False: Categorical({(True, 1.0): 0.3, (False, 2.0): 0.7}),\n",
    "            },\n",
    "            False: {\n",
    "                True: Categorical({(False, 1.0): 0.7, (True, 2.0): 0.3}),\n",
    "                False: Categorical({(False, 1.0): 0.3, (True, 2.0): 0.7}),\n",
    "            }\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTD_with_experience_replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstd(\n",
    "        transitions: Iterable[mrp.TransitionStep[S,A,R,S]],\n",
    "        actions: Callable[[S], Iterable[A]],\n",
    "        batch_size: int,\n",
    "        S: np.ndarray,\n",
    "        gradient: Callable,\n",
    "        replay_memory:np.ndarray,\n",
    "        feature_transform: Callable[[np.ndarray],np.ndarray],\n",
    "        γ: float\n",
    ") -> Iterator[FunctionApprox[Tuple[S, A]]]:\n",
    "    '''Return policies that try to maximize the reward based on the given\n",
    "    set of experiences.\n",
    "\n",
    "    Arguments:\n",
    "      transitions -- a sequence of state, action, reward, state (S, A, R, S')\n",
    "      S: state space\n",
    "      gradient: gradient matrix for feature space \n",
    "      batch_size: you know it!\n",
    "      actions -- a function returning the possible actions for a given state\n",
    "      feature_transform -- transform S vector to feature matrix\n",
    "      γ -- discount rate (0 < γ ≤ 1)\n",
    "\n",
    "    Returns:\n",
    "      Final weight matrix W\n",
    "\n",
    "    '''\n",
    "        W0=feature_transform(S0) # initial W based on first transition step\n",
    "        A=W0@(W0-gamma*gradient(W0)) # initial A based on first transition step\n",
    "        b=W0+batch[:,-1] # initial b based on first transition step\n",
    "        for _ in range(num_iter):\n",
    "            # append to memory\n",
    "            next_sample=next(transitions)\n",
    "            replay_memory=np.append(replay_memory,[next_sample.state,next_sample.next_state,next_sample.reward])\n",
    "            \n",
    "            number_of_rows = replay_memory.shape[0]\n",
    "            if len(number_of_rows)>batch_size:\n",
    "                random_indices = np.random.choice(number_of_rows, size=batch_size, replace=False)\n",
    "                batch = replay_memory[random_indices, :]\n",
    "                W=np.inverse(A)@b\n",
    "                A=A+feature_transform(batch)@(feature_transform(batch)-gamma*gradient(batch))\n",
    "                b=b+feature_transform(batch)+batch[:,-1]\n",
    "        \n",
    "    return np.inverse(A)@b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test LSTD with test_mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_0: Tabular[Tuple[bool, bool]] = Tabular(\n",
    "            {(s, a): 0.0\n",
    "             for s in test_mdp.states()\n",
    "             for a in test_mdp.actions(s)},\n",
    "            count_to_weight_func=lambda _: 0.1\n",
    "        )\n",
    "\n",
    "mrp=FlipFlop(0.7)\n",
    "start = Tabular(\n",
    "            {s: 0.0 for s in mrp.states()},\n",
    "            count_to_weight_func=lambda _: 0.1\n",
    "        )\n",
    "\n",
    "episode_length = 20\n",
    "episodes: Iterable[Iterable[mp.TransitionStep[bool]]] =\\\n",
    "            mrp.reward_traces(Choose({True, False}))\n",
    "\n",
    "transitions: Iterable[mp.TransitionStep[bool]] =\\\n",
    "            itertools.chain.from_iterable(\n",
    "                itertools.islice(episode, episode_length)\n",
    "                for episode in episodes\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = lstd(\n",
    "            transitions,\n",
    "            test_mdp.actions,\n",
    "            q_0,\n",
    "            γ=0.99\n",
    "        )\n",
    "next(qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lspi(\n",
    "        transitions: Iterable[mrp.TransitionStep[S,A,R,S]],\n",
    "        actions: Callable[[S], Iterable[A]],\n",
    "        batch_size: int,\n",
    "        S: np.ndarray,\n",
    "        gradient: Callable,\n",
    "        replay_memory:np.ndarray,\n",
    "        feature_transform: Callable[[np.ndarray],np.ndarray],\n",
    "        γ: float\n",
    ") -> Iterator[FunctionApprox[Tuple[S, A]]]:\n",
    "    '''Return policies that try to maximize the reward based on the given\n",
    "    set of experiences.\n",
    "\n",
    "    Arguments:\n",
    "      transitions -- a sequence of state, action, reward, state (S, A, R, S')\n",
    "      S: state space\n",
    "      gradient: gradient matrix for feature space \n",
    "      batch_size: you know it!\n",
    "      actions -- a function returning the possible actions for a given state\n",
    "      feature_transform -- transform S vector to feature matrix\n",
    "      γ -- discount rate (0 < γ ≤ 1)\n",
    "\n",
    "    Returns:\n",
    "      Final weight matrix W\n",
    "\n",
    "    '''\n",
    "        W0=feature_transform(S0) # initial W based on first transition step\n",
    "        A=W0@(W0-gamma*gradient(W0)) # initial A based on first transition step\n",
    "        b=W0+batch[:,-1] # initial b based on first transition step\n",
    "        eligi=gradient(W0)@(W@S) # this is the eligibility trace\n",
    "        for _ in range(num_iter):\n",
    "            # append to memory\n",
    "            next_sample=next(transitions)\n",
    "            replay_memory=np.append(replay_memory,[next_sample.state,next_sample.next_state,next_sample.reward])\n",
    "            \n",
    "            number_of_rows = replay_memory.shape[0]\n",
    "            if len(number_of_rows)>batch_size:\n",
    "                random_indices = np.random.choice(number_of_rows, size=batch_size, replace=False)\n",
    "                batch = replay_memory[random_indices, :]\n",
    "                W=np.inverse(A)@b\n",
    "                A=A+E@feature_transform(batch)@(feature_transform(batch)-gamma*gradient(batch))\n",
    "                b=b+feature_transform(batch)+E@batch[:,-1]\n",
    "        \n",
    "    return np.inverse(A)@b\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
